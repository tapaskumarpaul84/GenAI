{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "036805e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "all ok\n"
     ]
    }
   ],
   "source": [
    "print(\"all ok\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "48984578",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def pretty_print_docs(docs):\n",
    "    print(\n",
    "        f\"\\n{'-' * 100}\\n\".join(\n",
    "            [\n",
    "                f\"Document {i+1}:\\n\\n{d.page_content}\\nMetadata: {d.metadata}\"\n",
    "                for i, d in enumerate(docs)\n",
    "            ]\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "26b4c900",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "query = \"How to speedup LLMs?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "576bfe9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "passages = [\n",
    "   {\n",
    "      \"id\":1,\n",
    "      \"text\":\"Introduce *lookahead decoding*: - a parallel decoding algo to accelerate LLM inference - w/o the need for a draft model or a data store - linearly decreases # decoding steps relative to log(FLOPs) used per decoding step.\",\n",
    "      \"meta\": {\"additional\": \"info1\"}\n",
    "   },\n",
    "   {\n",
    "      \"id\":2,\n",
    "      \"text\":\"LLM inference efficiency will be one of the most crucial topics for both industry and academia, simply because the more efficient you are, the more $$$ you will save. vllm project is a must-read for this direction, and now they have just released the paper\",\n",
    "      \"meta\": {\"additional\": \"info2\"}\n",
    "   },\n",
    "   {\n",
    "      \"id\":3,\n",
    "      \"text\":\"There are many ways to increase LLM inference throughput (tokens/second) and decrease memory footprint, sometimes at the same time. Here are a few methods I’ve found effective when working with Llama 2. These methods are all well-integrated with Hugging Face. This list is far from exhaustive; some of these techniques can be used in combination with each other and there are plenty of others to try. - Bettertransformer (Optimum Library): Simply call `model.to_bettertransformer()` on your Hugging Face model for a modest improvement in tokens per second. - Fp4 Mixed-Precision (Bitsandbytes): Requires minimal configuration and dramatically reduces the model's memory footprint. - AutoGPTQ: Time-consuming but leads to a much smaller model and faster inference. The quantization is a one-time cost that pays off in the long run.\",\n",
    "      \"meta\": {\"additional\": \"info3\"}\n",
    "\n",
    "   },\n",
    "   {\n",
    "      \"id\":4,\n",
    "      \"text\":\"Ever want to make your LLM inference go brrrrr but got stuck at implementing speculative decoding and finding the suitable draft model? No more pain! Thrilled to unveil Medusa, a simple framework that removes the annoying draft model while getting 2x speedup.\",\n",
    "      \"meta\": {\"additional\": \"info4\"}\n",
    "   },\n",
    "   {\n",
    "      \"id\":5,\n",
    "      \"text\":\"vLLM is a fast and easy-to-use library for LLM inference and serving. vLLM is fast with: State-of-the-art serving throughput Efficient management of attention key and value memory with PagedAttention Continuous batching of incoming requests Optimized CUDA kernels\",\n",
    "      \"meta\": {\"additional\": \"info5\"}\n",
    "   }\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1f68de89",
   "metadata": {},
   "outputs": [],
   "source": [
    "from flashrank.Ranker import Ranker,RerankRequest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bd1174da",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_result(query,passages,choice):\n",
    "    if choice=='Nano':\n",
    "        ranker=Ranker()\n",
    "    elif choice=='Small':\n",
    "        ranker=Ranker(model_name='ms-marco-MiniLM-L-12-v2',cache_dir=r'../Advance_Rag/Opt')\n",
    "    elif choice=='Medium':\n",
    "        ranker=Ranker(model_name='rank-T5-flan',cache_dir=r'../Advance_Rag/Opt')\n",
    "    elif choice=='Large':\n",
    "        ranker=Ranker(model_name='ms-marco-MultiBERT-L-12',cache_dir=r'../Advance_Rag/Opt')\n",
    "    reranker_request=RerankRequest(query=query,passages=passages)\n",
    "    results=ranker.rerank(reranker_request)\n",
    "\n",
    "    print(results)\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c3743623",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:flashrank.Ranker:Downloading ms-marco-TinyBERT-L-2-v2...\n",
      "ms-marco-TinyBERT-L-2-v2.zip: 100%|██████████| 3.26M/3.26M [00:01<00:00, 3.10MiB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'id': 4, 'text': 'Ever want to make your LLM inference go brrrrr but got stuck at implementing speculative decoding and finding the suitable draft model? No more pain! Thrilled to unveil Medusa, a simple framework that removes the annoying draft model while getting 2x speedup.', 'meta': {'additional': 'info4'}, 'score': 0.018163264}, {'id': 5, 'text': 'vLLM is a fast and easy-to-use library for LLM inference and serving. vLLM is fast with: State-of-the-art serving throughput Efficient management of attention key and value memory with PagedAttention Continuous batching of incoming requests Optimized CUDA kernels', 'meta': {'additional': 'info5'}, 'score': 0.013987866}, {'id': 3, 'text': \"There are many ways to increase LLM inference throughput (tokens/second) and decrease memory footprint, sometimes at the same time. Here are a few methods I’ve found effective when working with Llama 2. These methods are all well-integrated with Hugging Face. This list is far from exhaustive; some of these techniques can be used in combination with each other and there are plenty of others to try. - Bettertransformer (Optimum Library): Simply call `model.to_bettertransformer()` on your Hugging Face model for a modest improvement in tokens per second. - Fp4 Mixed-Precision (Bitsandbytes): Requires minimal configuration and dramatically reduces the model's memory footprint. - AutoGPTQ: Time-consuming but leads to a much smaller model and faster inference. The quantization is a one-time cost that pays off in the long run.\", 'meta': {'additional': 'info3'}, 'score': 0.00091874925}, {'id': 1, 'text': 'Introduce *lookahead decoding*: - a parallel decoding algo to accelerate LLM inference - w/o the need for a draft model or a data store - linearly decreases # decoding steps relative to log(FLOPs) used per decoding step.', 'meta': {'additional': 'info1'}, 'score': 0.00076141005}, {'id': 2, 'text': 'LLM inference efficiency will be one of the most crucial topics for both industry and academia, simply because the more efficient you are, the more $$$ you will save. vllm project is a must-read for this direction, and now they have just released the paper', 'meta': {'additional': 'info2'}, 'score': 0.0002851765}]\n",
      "CPU times: total: 984 ms\n",
      "Wall time: 4.15 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'id': 4,\n",
       "  'text': 'Ever want to make your LLM inference go brrrrr but got stuck at implementing speculative decoding and finding the suitable draft model? No more pain! Thrilled to unveil Medusa, a simple framework that removes the annoying draft model while getting 2x speedup.',\n",
       "  'meta': {'additional': 'info4'},\n",
       "  'score': 0.018163264},\n",
       " {'id': 5,\n",
       "  'text': 'vLLM is a fast and easy-to-use library for LLM inference and serving. vLLM is fast with: State-of-the-art serving throughput Efficient management of attention key and value memory with PagedAttention Continuous batching of incoming requests Optimized CUDA kernels',\n",
       "  'meta': {'additional': 'info5'},\n",
       "  'score': 0.013987866},\n",
       " {'id': 3,\n",
       "  'text': \"There are many ways to increase LLM inference throughput (tokens/second) and decrease memory footprint, sometimes at the same time. Here are a few methods I’ve found effective when working with Llama 2. These methods are all well-integrated with Hugging Face. This list is far from exhaustive; some of these techniques can be used in combination with each other and there are plenty of others to try. - Bettertransformer (Optimum Library): Simply call `model.to_bettertransformer()` on your Hugging Face model for a modest improvement in tokens per second. - Fp4 Mixed-Precision (Bitsandbytes): Requires minimal configuration and dramatically reduces the model's memory footprint. - AutoGPTQ: Time-consuming but leads to a much smaller model and faster inference. The quantization is a one-time cost that pays off in the long run.\",\n",
       "  'meta': {'additional': 'info3'},\n",
       "  'score': 0.00091874925},\n",
       " {'id': 1,\n",
       "  'text': 'Introduce *lookahead decoding*: - a parallel decoding algo to accelerate LLM inference - w/o the need for a draft model or a data store - linearly decreases # decoding steps relative to log(FLOPs) used per decoding step.',\n",
       "  'meta': {'additional': 'info1'},\n",
       "  'score': 0.00076141005},\n",
       " {'id': 2,\n",
       "  'text': 'LLM inference efficiency will be one of the most crucial topics for both industry and academia, simply because the more efficient you are, the more $$$ you will save. vllm project is a must-read for this direction, and now they have just released the paper',\n",
       "  'meta': {'additional': 'info2'},\n",
       "  'score': 0.0002851765}]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "get_result(query,passages,'Nano')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d8c7175b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:flashrank.Ranker:Downloading ms-marco-MiniLM-L-12-v2...\n",
      "ms-marco-MiniLM-L-12-v2.zip: 100%|██████████| 21.6M/21.6M [00:06<00:00, 3.77MiB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'id': 4, 'text': 'Ever want to make your LLM inference go brrrrr but got stuck at implementing speculative decoding and finding the suitable draft model? No more pain! Thrilled to unveil Medusa, a simple framework that removes the annoying draft model while getting 2x speedup.', 'meta': {'additional': 'info4'}, 'score': 0.9439003}, {'id': 3, 'text': \"There are many ways to increase LLM inference throughput (tokens/second) and decrease memory footprint, sometimes at the same time. Here are a few methods I’ve found effective when working with Llama 2. These methods are all well-integrated with Hugging Face. This list is far from exhaustive; some of these techniques can be used in combination with each other and there are plenty of others to try. - Bettertransformer (Optimum Library): Simply call `model.to_bettertransformer()` on your Hugging Face model for a modest improvement in tokens per second. - Fp4 Mixed-Precision (Bitsandbytes): Requires minimal configuration and dramatically reduces the model's memory footprint. - AutoGPTQ: Time-consuming but leads to a much smaller model and faster inference. The quantization is a one-time cost that pays off in the long run.\", 'meta': {'additional': 'info3'}, 'score': 0.8475677}, {'id': 1, 'text': 'Introduce *lookahead decoding*: - a parallel decoding algo to accelerate LLM inference - w/o the need for a draft model or a data store - linearly decreases # decoding steps relative to log(FLOPs) used per decoding step.', 'meta': {'additional': 'info1'}, 'score': 0.8068305}, {'id': 5, 'text': 'vLLM is a fast and easy-to-use library for LLM inference and serving. vLLM is fast with: State-of-the-art serving throughput Efficient management of attention key and value memory with PagedAttention Continuous batching of incoming requests Optimized CUDA kernels', 'meta': {'additional': 'info5'}, 'score': 0.5689538}, {'id': 2, 'text': 'LLM inference efficiency will be one of the most crucial topics for both industry and academia, simply because the more efficient you are, the more $$$ you will save. vllm project is a must-read for this direction, and now they have just released the paper', 'meta': {'additional': 'info2'}, 'score': 0.0044068173}]\n",
      "CPU times: total: 4.75 s\n",
      "Wall time: 9.78 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'id': 4,\n",
       "  'text': 'Ever want to make your LLM inference go brrrrr but got stuck at implementing speculative decoding and finding the suitable draft model? No more pain! Thrilled to unveil Medusa, a simple framework that removes the annoying draft model while getting 2x speedup.',\n",
       "  'meta': {'additional': 'info4'},\n",
       "  'score': 0.9439003},\n",
       " {'id': 3,\n",
       "  'text': \"There are many ways to increase LLM inference throughput (tokens/second) and decrease memory footprint, sometimes at the same time. Here are a few methods I’ve found effective when working with Llama 2. These methods are all well-integrated with Hugging Face. This list is far from exhaustive; some of these techniques can be used in combination with each other and there are plenty of others to try. - Bettertransformer (Optimum Library): Simply call `model.to_bettertransformer()` on your Hugging Face model for a modest improvement in tokens per second. - Fp4 Mixed-Precision (Bitsandbytes): Requires minimal configuration and dramatically reduces the model's memory footprint. - AutoGPTQ: Time-consuming but leads to a much smaller model and faster inference. The quantization is a one-time cost that pays off in the long run.\",\n",
       "  'meta': {'additional': 'info3'},\n",
       "  'score': 0.8475677},\n",
       " {'id': 1,\n",
       "  'text': 'Introduce *lookahead decoding*: - a parallel decoding algo to accelerate LLM inference - w/o the need for a draft model or a data store - linearly decreases # decoding steps relative to log(FLOPs) used per decoding step.',\n",
       "  'meta': {'additional': 'info1'},\n",
       "  'score': 0.8068305},\n",
       " {'id': 5,\n",
       "  'text': 'vLLM is a fast and easy-to-use library for LLM inference and serving. vLLM is fast with: State-of-the-art serving throughput Efficient management of attention key and value memory with PagedAttention Continuous batching of incoming requests Optimized CUDA kernels',\n",
       "  'meta': {'additional': 'info5'},\n",
       "  'score': 0.5689538},\n",
       " {'id': 2,\n",
       "  'text': 'LLM inference efficiency will be one of the most crucial topics for both industry and academia, simply because the more efficient you are, the more $$$ you will save. vllm project is a must-read for this direction, and now they have just released the paper',\n",
       "  'meta': {'additional': 'info2'},\n",
       "  'score': 0.0044068173}]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "get_result(query,passages,'Small')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "027657a3",
   "metadata": {},
   "source": [
    "#### Flash Rerank on data using Langchain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "47643cb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import TextLoader\n",
    "\n",
    "loader=TextLoader(r'..\\data\\state_of_the_union.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "9c65d4bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "docs=loader.load()\n",
    "print(len(docs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "9a3f4889",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "splitter=RecursiveCharacterTextSplitter(chunk_size=300,chunk_overlap=60)\n",
    "split_docs=splitter.split_documents(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "951511f0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "171"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(split_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "ff500cc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "for id ,text in enumerate(split_docs):\n",
    "    text.metadata[\"id\"]=id\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4181efd5",
   "metadata": {},
   "source": [
    "### Embedding model and llm config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "62688152",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from langchain_openai.embeddings import OpenAIEmbeddings\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "load_dotenv()\n",
    "os.environ[\"OPENAI_API_KEY\"]=os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "embeddings=OpenAIEmbeddings(model=\"text-embedding-ada-002\")\n",
    "llm=ChatOpenAI(model='gpt-4o')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "230fd014",
   "metadata": {},
   "source": [
    "### VectorDB and Retriever setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "0bb17dad",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.vectorstores import FAISS\n",
    "\n",
    "vectorstore=FAISS.from_documents(split_docs,embeddings)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "86991e93",
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever=vectorstore.as_retriever(search_type='similarity',search_kwargs={\"k\":10})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "df05fee2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n"
     ]
    }
   ],
   "source": [
    "re_docs=retriever.invoke(\"What did the president say about Ketanji Brown Jackson\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "f69173b8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(id='1aca980e-236e-4a71-af88-652ec4932157', metadata={'source': '..\\\\data\\\\state_of_the_union.txt', 'id': 130}, page_content='And I did that 4 days ago, when I nominated Circuit Court of Appeals Judge Ketanji Brown Jackson. One of our nation’s top legal minds, who will continue Justice Breyer’s legacy of excellence.'),\n",
       " Document(id='155f8916-a26a-4deb-b09a-4e467cb4034f', metadata={'source': '..\\\\data\\\\state_of_the_union.txt', 'id': 140}, page_content='As I said last year, especially to our younger transgender Americans, I will always have your back as your President, so you can be yourself and reach your God-given potential.'),\n",
       " Document(id='43195757-0b56-43f5-885e-d81fc3e5c0fb', metadata={'source': '..\\\\data\\\\state_of_the_union.txt', 'id': 116}, page_content='We can’t change how divided we’ve been. But we can change how we move forward—on COVID-19 and other issues we must face together. \\n\\nI recently visited the New York City Police Department days after the funerals of Officer Wilbert Mora and his partner, Officer Jason Rivera.'),\n",
       " Document(id='70b11b0a-3c2a-4e2a-9ced-415a8dece20e', metadata={'source': '..\\\\data\\\\state_of_the_union.txt', 'id': 49}, page_content='As I’ve told Xi Jinping, it is never a good bet to bet against the American people. \\n\\nWe’ll create good jobs for millions of Americans, modernizing roads, airports, ports, and waterways all across America.'),\n",
       " Document(id='b5ee5ffc-57d3-49fd-a0ef-e08a0664fd92', metadata={'source': '..\\\\data\\\\state_of_the_union.txt', 'id': 44}, page_content='But that trickle-down theory led to weaker economic growth, lower wages, bigger deficits, and the widest gap between those at the top and everyone else in nearly a century. \\n\\nVice President Harris and I ran for office with a new economic vision for America.'),\n",
       " Document(id='91abbc48-3b36-431d-80b1-7537f943ba62', metadata={'source': '..\\\\data\\\\state_of_the_union.txt', 'id': 65}, page_content='All told, we created 369,000 new manufacturing jobs in America just last year. \\n\\nPowered by people I’ve met like JoJo Burgess, from generations of union steelworkers from Pittsburgh, who’s here with us tonight. \\n\\nAs Ohio Senator Sherrod Brown says, “It’s time to bury the label “Rust Belt.”'),\n",
       " Document(id='7023d829-2c4b-47cf-a081-fc527673b077', metadata={'source': '..\\\\data\\\\state_of_the_union.txt', 'id': 0}, page_content='Madam Speaker, Madam Vice President, our First Lady and Second Gentleman. Members of Congress and the Cabinet. Justices of the Supreme Court. My fellow Americans.  \\n\\nLast year COVID-19 kept us apart. This year we are finally together again.'),\n",
       " Document(id='4d149b85-2b6c-433b-83e4-749b81d724e0', metadata={'source': '..\\\\data\\\\state_of_the_union.txt', 'id': 118}, page_content='I spoke with their families and told them that we are forever in debt for their sacrifice, and we will carry on their mission to restore the trust and safety every community deserves. \\n\\nI’ve worked on these issues a long time.'),\n",
       " Document(id='1117238d-5cbb-4ff7-83f7-7b074c95a009', metadata={'source': '..\\\\data\\\\state_of_the_union.txt', 'id': 161}, page_content='Last month, I announced our plan to supercharge  \\nthe Cancer Moonshot that President Obama asked me to lead six years ago. \\n\\nOur goal is to cut the cancer death rate by at least 50% over the next 25 years, turn more cancers from death sentences into treatable diseases.'),\n",
       " Document(id='a0d5b1c6-5f9d-4c0a-9450-0f1138149841', metadata={'source': '..\\\\data\\\\state_of_the_union.txt', 'id': 131}, page_content='A former top litigator in private practice. A former federal public defender. And from a family of public school educators and police officers. A consensus builder. Since she’s been nominated, she’s received a broad range of support—from the Fraternal Order of Police to former judges appointed by')]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "8760f3d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document 1:\n",
      "\n",
      "And I did that 4 days ago, when I nominated Circuit Court of Appeals Judge Ketanji Brown Jackson. One of our nation’s top legal minds, who will continue Justice Breyer’s legacy of excellence.\n",
      "Metadata: {'source': '..\\\\data\\\\state_of_the_union.txt', 'id': 130}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Document 2:\n",
      "\n",
      "As I said last year, especially to our younger transgender Americans, I will always have your back as your President, so you can be yourself and reach your God-given potential.\n",
      "Metadata: {'source': '..\\\\data\\\\state_of_the_union.txt', 'id': 140}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Document 3:\n",
      "\n",
      "We can’t change how divided we’ve been. But we can change how we move forward—on COVID-19 and other issues we must face together. \n",
      "\n",
      "I recently visited the New York City Police Department days after the funerals of Officer Wilbert Mora and his partner, Officer Jason Rivera.\n",
      "Metadata: {'source': '..\\\\data\\\\state_of_the_union.txt', 'id': 116}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Document 4:\n",
      "\n",
      "As I’ve told Xi Jinping, it is never a good bet to bet against the American people. \n",
      "\n",
      "We’ll create good jobs for millions of Americans, modernizing roads, airports, ports, and waterways all across America.\n",
      "Metadata: {'source': '..\\\\data\\\\state_of_the_union.txt', 'id': 49}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Document 5:\n",
      "\n",
      "But that trickle-down theory led to weaker economic growth, lower wages, bigger deficits, and the widest gap between those at the top and everyone else in nearly a century. \n",
      "\n",
      "Vice President Harris and I ran for office with a new economic vision for America.\n",
      "Metadata: {'source': '..\\\\data\\\\state_of_the_union.txt', 'id': 44}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Document 6:\n",
      "\n",
      "All told, we created 369,000 new manufacturing jobs in America just last year. \n",
      "\n",
      "Powered by people I’ve met like JoJo Burgess, from generations of union steelworkers from Pittsburgh, who’s here with us tonight. \n",
      "\n",
      "As Ohio Senator Sherrod Brown says, “It’s time to bury the label “Rust Belt.”\n",
      "Metadata: {'source': '..\\\\data\\\\state_of_the_union.txt', 'id': 65}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Document 7:\n",
      "\n",
      "Madam Speaker, Madam Vice President, our First Lady and Second Gentleman. Members of Congress and the Cabinet. Justices of the Supreme Court. My fellow Americans.  \n",
      "\n",
      "Last year COVID-19 kept us apart. This year we are finally together again.\n",
      "Metadata: {'source': '..\\\\data\\\\state_of_the_union.txt', 'id': 0}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Document 8:\n",
      "\n",
      "I spoke with their families and told them that we are forever in debt for their sacrifice, and we will carry on their mission to restore the trust and safety every community deserves. \n",
      "\n",
      "I’ve worked on these issues a long time.\n",
      "Metadata: {'source': '..\\\\data\\\\state_of_the_union.txt', 'id': 118}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Document 9:\n",
      "\n",
      "Last month, I announced our plan to supercharge  \n",
      "the Cancer Moonshot that President Obama asked me to lead six years ago. \n",
      "\n",
      "Our goal is to cut the cancer death rate by at least 50% over the next 25 years, turn more cancers from death sentences into treatable diseases.\n",
      "Metadata: {'source': '..\\\\data\\\\state_of_the_union.txt', 'id': 161}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Document 10:\n",
      "\n",
      "A former top litigator in private practice. A former federal public defender. And from a family of public school educators and police officers. A consensus builder. Since she’s been nominated, she’s received a broad range of support—from the Fraternal Order of Police to former judges appointed by\n",
      "Metadata: {'source': '..\\\\data\\\\state_of_the_union.txt', 'id': 131}\n"
     ]
    }
   ],
   "source": [
    "pretty_print_docs(re_docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ede88bdf",
   "metadata": {},
   "source": [
    "### Compression retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "6ea5ceb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.retrievers import ContextualCompressionRetriever\n",
    "from langchain.retrievers.document_compressors import FlashrankRerank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "346db0b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:flashrank.Ranker:Downloading ms-marco-MultiBERT-L-12...\n",
      "ms-marco-MultiBERT-L-12.zip: 100%|██████████| 98.7M/98.7M [00:24<00:00, 4.25MiB/s]\n"
     ]
    }
   ],
   "source": [
    "compressor=FlashrankRerank()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "5ce59e59",
   "metadata": {},
   "outputs": [],
   "source": [
    "compression_retriever=ContextualCompressionRetriever(base_compressor=compressor,\n",
    "                                                     base_retriever=retriever)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "3cc1ac91",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n"
     ]
    }
   ],
   "source": [
    "compressed_docs=compression_retriever.invoke(\"What did the president say about Ketanji Brown Jackson\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "62ce5dba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(compressed_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "b0a9abc7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'id': 130, 'relevance_score': 0.9982564, 'source': '..\\\\data\\\\state_of_the_union.txt'}, page_content='And I did that 4 days ago, when I nominated Circuit Court of Appeals Judge Ketanji Brown Jackson. One of our nation’s top legal minds, who will continue Justice Breyer’s legacy of excellence.'),\n",
       " Document(metadata={'id': 44, 'relevance_score': 0.82325286, 'source': '..\\\\data\\\\state_of_the_union.txt'}, page_content='But that trickle-down theory led to weaker economic growth, lower wages, bigger deficits, and the widest gap between those at the top and everyone else in nearly a century. \\n\\nVice President Harris and I ran for office with a new economic vision for America.'),\n",
       " Document(metadata={'id': 161, 'relevance_score': 0.21324193, 'source': '..\\\\data\\\\state_of_the_union.txt'}, page_content='Last month, I announced our plan to supercharge  \\nthe Cancer Moonshot that President Obama asked me to lead six years ago. \\n\\nOur goal is to cut the cancer death rate by at least 50% over the next 25 years, turn more cancers from death sentences into treatable diseases.')]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compressed_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "9c1ba17e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document 1:\n",
      "\n",
      "And I did that 4 days ago, when I nominated Circuit Court of Appeals Judge Ketanji Brown Jackson. One of our nation’s top legal minds, who will continue Justice Breyer’s legacy of excellence.\n",
      "Metadata: {'id': 130, 'relevance_score': 0.9982564, 'source': '..\\\\data\\\\state_of_the_union.txt'}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Document 2:\n",
      "\n",
      "But that trickle-down theory led to weaker economic growth, lower wages, bigger deficits, and the widest gap between those at the top and everyone else in nearly a century. \n",
      "\n",
      "Vice President Harris and I ran for office with a new economic vision for America.\n",
      "Metadata: {'id': 44, 'relevance_score': 0.82325286, 'source': '..\\\\data\\\\state_of_the_union.txt'}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Document 3:\n",
      "\n",
      "Last month, I announced our plan to supercharge  \n",
      "the Cancer Moonshot that President Obama asked me to lead six years ago. \n",
      "\n",
      "Our goal is to cut the cancer death rate by at least 50% over the next 25 years, turn more cancers from death sentences into treatable diseases.\n",
      "Metadata: {'id': 161, 'relevance_score': 0.21324193, 'source': '..\\\\data\\\\state_of_the_union.txt'}\n"
     ]
    }
   ],
   "source": [
    "pretty_print_docs(compressed_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "2aa84c8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[130, 44, 161]\n"
     ]
    }
   ],
   "source": [
    "print([doc.metadata[\"id\"] for doc in compressed_docs])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87e2ccd5",
   "metadata": {},
   "source": [
    "### Chain config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "aa8c08ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import PromptTemplate\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "6dda27e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt=PromptTemplate.from_template(template=\"\"\"You are a helpful assistant.\n",
    "                                    answer the user query based on context. If you do not\n",
    "                                    find any solid response for the query response that \n",
    "                                    you don't know.\n",
    "                                    context:{context}\n",
    "                                    question: {query}\n",
    "                                    Answer:\n",
    "                                    \"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "ccc5d52f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_docs(docs):\n",
    "    return '\\n\\n'.join(doc.page_content for doc in docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "d36715ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "chain=(\n",
    "    {\"context\": compression_retriever | format_docs,\"query\":RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "919ec40b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'The president stated that he nominated Circuit Court of Appeals Judge Ketanji Brown Jackson and described her as one of the nation’s top legal minds who will continue Justice Breyer’s legacy of excellence.'"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response=chain.invoke(\"What did the president say about Ketanji Brown Jackson\")\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6eeaa021",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"'The president stated that he nominated Circuit Court of Appeals Judge Ketanji Brown Jackson'\n",
    "' and described her as one of the nation’s top legal minds who will continue Justice '\n",
    "'Breyer’s legacy of excellence.'\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "2c4f017f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"The USA has stated that Russia's attack on Ukraine was premeditated and unprovoked, emphasizing that American diplomacy and resolve are important. The statement reinforces that the USA will continue to aid the Ukrainian people as they defend their country and will work to ease their suffering. However, it is made clear that American forces are not and will not be engaged in conflict with Russian forces in Ukraine.\""
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response=chain.invoke(\"USA statement on ukraine\")\n",
    "response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4481093",
   "metadata": {},
   "source": [
    "### response without Reranking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "6b5264a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "simple_chain=(\n",
    "    {\"context\": retriever | format_docs,\"query\":RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "ae72cb7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'The president mentioned that he nominated Circuit Court of Appeals Judge Ketanji Brown Jackson, describing her as one of the nation’s top legal minds who will continue Justice Breyer’s legacy of excellence.'"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response=simple_chain.invoke(\"What did the president say about Ketanji Brown Jackson\")\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f3ad3d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "'The president mentioned that he nominated Circuit Court of Appeals Judge Ketanji Brown '\n",
    "'Jackson, describing her as one of the nation’s top legal minds who will continue '\n",
    "'Justice Breyer’s legacy of excellence.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "52116be2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'The United States has made it clear that while its forces will not engage in conflict with Russian forces in Ukraine, it stands firmly with the Ukrainian people. The U.S. is providing over $1 billion in military, economic, and humanitarian assistance to Ukraine, in solidarity with the country’s fight for freedom. Additionally, the U.S. emphasizes the importance of defending NATO Allies and has pledged to use its full power to protect NATO territory. The U.S. position also highlights that despite any battlefield gains by Russia, the long-term consequences will weaken Russia and strengthen the rest of the world.'"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response=simple_chain.invoke(\"USA statement on ukraine\")\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d391741",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "genai_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
