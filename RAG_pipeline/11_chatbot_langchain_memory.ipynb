{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "38c8f88f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "all ok\n"
     ]
    }
   ],
   "source": [
    "print(\"all ok\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d8b54e25",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "os.environ[\"GOOGLE_API_KEY\"]=os.getenv(\"GOOGLE_API_KEY\")\n",
    "os.environ[\"OPENAI_API_KEY\"]=os.getenv(\"OPENAI_API_KEY\")\n",
    "os.environ[\"LANGSMITH_TRACING\"]=os.getenv(\"LANGSMITH_TRACING\")\n",
    "os.environ[\"LANGSMITH_ENDPOINT\"]=os.getenv(\"LANGSMITH_ENDPOINT\")\n",
    "os.environ[\"LANGSMITH_API_KEY\"]=os.getenv(\"LANGSMITH_API_KEY\")\n",
    "os.environ[\"LANGSMITH_PROJECT\"]=os.getenv(\"LANGSMITH_PROJECT\")\n",
    "os.environ[\"QROQ_API_KEY\"]= os.getenv(\"QROQ_API_KEY\")\n",
    "os.environ[\"HF_API_TOKEN\"]=os.getenv(\"HF_API_TOKEN\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7d1c9853",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e60186d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Hp\\miniconda3\\envs\\genai_venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "embedding_model=HuggingFaceEmbeddings(model_name='sentence-transformers/all-mpnet-base-v2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "93c32440",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "768"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(embedding_model.embed_query(\"Hello how are you\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "934c4447",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai.embeddings import OpenAIEmbeddings\n",
    "\n",
    "openai_embedding=OpenAIEmbeddings( model = \"text-embedding-ada-002\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1b83f92f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1536"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(openai_embedding.embed_query(\"Hello how are you\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a28254e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "\n",
    "llm=ChatGoogleGenerativeAI(model='gemini-2.5-flash')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "02898d22",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'As an AI, I don\\'t have feelings or a physical state like humans do, so I don\\'t experience \"being\" in the same way.\\n\\nHowever, I am functioning perfectly and ready to assist you!\\n\\nHow can I help you today?'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response=llm.invoke(\"How are you\")\n",
    "response.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3ed35ebd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "parser= StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "97f2210d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import HumanMessage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a87bcfd6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hi there! How can I help you today?\n",
      "Good bye have a great day.\n"
     ]
    }
   ],
   "source": [
    "#simple QnA system\n",
    "while True:\n",
    "    message=input(\"Enter your query:\")\n",
    "    if message == 'bye':\n",
    "        print(\"Good bye have a great day.\")\n",
    "        break\n",
    "    else:\n",
    "        print(parser.invoke(llm.invoke([HumanMessage(content=message)])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "085e9cb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your name is Tapas. You told me that in your first message.\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.messages import AIMessage\n",
    "\n",
    "result=llm.invoke(\n",
    "    [\n",
    "        HumanMessage(content=\"Hello I am tapas.\"),\n",
    "        AIMessage(content=\"Hello Tapas, How can help you today?\"),\n",
    "        HumanMessage(content=\"What is my name?\")\n",
    "    \n",
    "    ]\n",
    ")\n",
    "print(parser.invoke(result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5f80e0e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.chat_history import BaseChatMessageHistory,InMemoryChatMessageHistory\n",
    "from langchain_core.runnables.history import RunnableWithMessageHistory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ed5d2286",
   "metadata": {},
   "outputs": [],
   "source": [
    "store={}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "93a75110",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_session_history(session_id:str)->BaseChatMessageHistory:\n",
    "    if session_id not in store:\n",
    "        store[session_id]=InMemoryChatMessageHistory()\n",
    "    return store[session_id]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "caf970c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_with_memory=RunnableWithMessageHistory(llm,get_session_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d7ac6a95",
   "metadata": {},
   "outputs": [],
   "source": [
    "config={\"configurable\":{\"session_id\":\"first_chat\"}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "97d16039",
   "metadata": {},
   "outputs": [],
   "source": [
    "response=model_with_memory.invoke([HumanMessage(content=\"Hello My name is Tapas\")],config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b5076b64",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Hello Tapas! It's nice to meet you.\""
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "cf5085b6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hello Tapas! That\\'s a great question, as RAM is one of the most crucial components in any laptop.\\n\\n**RAM** stands for **R**andom **A**ccess **M**emory.\\n\\nThink of your laptop like this:\\n\\n*   **Your Hard Drive (HDD) or Solid State Drive (SSD)** is like a **filing cabinet** or a **library**. It\\'s where all your programs, documents, photos, and the operating system are permanently stored. It\\'s vast, but accessing things from it takes time.\\n*   **Your Laptop\\'s Processor (CPU)** is like your **brain**. It does all the thinking and calculations.\\n*   **RAM** is like your **desk or workbench**. When you decide to work on something (open a program, edit a document, browse the web), your laptop pulls the necessary data from the slow \"filing cabinet\" (storage) and places it onto the super-fast \"desk\" (RAM).\\n\\n**Here\\'s a breakdown of what RAM is and what it does:**\\n\\n1.  **Temporary, High-Speed Storage:** RAM is a very fast type of memory that your laptop uses to store data that it\\'s actively working with *right now*. Unlike your hard drive, it\\'s not for long-term storage.\\n2.  **The \"Working Space\":** When you open an application (like a web browser, a word processor, or a game), the laptop loads parts of that application into RAM. This allows the CPU to access that data almost instantly, making your laptop feel responsive.\\n3.  **Multitasking Power:** The more RAM you have, the more \"workspace\" your laptop has. This means you can have more applications open simultaneously, switch between them smoothly, and run demanding programs without your laptop slowing down.\\n4.  **Volatile Memory:** This is a key characteristic. When you turn off your laptop, everything stored in RAM is erased. That\\'s why you need to save your work to your hard drive or SSD before shutting down!\\n\\n**Why is RAM important?**\\n\\n*   **Speed:** RAM is significantly faster than your hard drive or SSD. If your laptop had to constantly pull data directly from the storage drive, it would be incredibly slow.\\n*   **Responsiveness:** Adequate RAM ensures that your applications load quickly, tabs in your browser don\\'t constantly reload, and your overall computing experience is smooth.\\n*   **Multitasking:** If you\\'re someone who likes to have many browser tabs open, several applications running, or you edit videos/photos, more RAM is essential.\\n*   **Gaming:** Games are very demanding on RAM as they need to store a lot of textures, levels, and game data for quick access.\\n\\n**How much RAM do you need?**\\n\\n*   **4GB:** Very basic use (light browsing, email, simple documents). Becoming less common and often too little for a good experience.\\n*   **8GB:** Good for most everyday users (web browsing, office work, streaming, light photo editing). This is a common sweet spot.\\n*   **16GB:** Excellent for power users, gamers, content creators (video editing, graphic design), and those who run many applications simultaneously.\\n*   **32GB+:** For professional-grade work, heavy gaming, virtual machines, and specialized scientific or engineering applications.\\n\\nIn summary, RAM is your laptop\\'s short-term, super-fast memory that allows it to efficiently run programs and manage multiple tasks. It\\'s crucial for a smooth and responsive computing experience!'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response=model_with_memory.invoke([HumanMessage(content=\"What is ram in laptop?\")],config=config)\n",
    "response.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d138e18e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Yes, Tapas, you can often increase the RAM in a laptop, but it depends on your specific laptop model. It\\'s one of the most common and effective upgrades you can make to improve performance, especially for multitasking and demanding applications.\\n\\nHere\\'s how you can go about it:\\n\\n### Crucial First Steps: Determine if your RAM is Upgradeable\\n\\nBefore you buy anything, you need to figure out a few things:\\n\\n1.  **Is the RAM Soldered?** Some modern, ultra-slim laptops (especially ultrabooks, MacBooks, and some budget models) have their RAM soldered directly onto the motherboard. If this is the case, it\\'s **not upgradeable** at all.\\n2.  **How Many RAM Slots Do You Have?** Most laptops have either one or two RAM slots.\\n3.  **What Type of RAM Does Your Laptop Use?** (DDR3, DDR4, DDR5) and what is its speed (MHz)?\\n4.  **What is the Maximum RAM Your Laptop Supports?** Every motherboard has a limit.\\n5.  **What RAM is Currently Installed?** (Capacity per stick, e.g., 4GB, 8GB).\\n\\n### How to Find This Information:\\n\\n#### 1. Software Tools (Easiest Way)\\n\\n*   **Windows Task Manager (Windows 10/11):**\\n    *   Right-click on the Taskbar and select \"Task Manager\" (or press Ctrl+Shift+Esc).\\n    *   Go to the \"Performance\" tab.\\n    *   Click on \"Memory.\"\\n    *   Here you\\'ll see the total RAM, speed, form factor (SODIMM), and how many slots are \"Used\" out of the total available. This will tell you if you have an empty slot.\\n*   **CPU-Z (Free Third-Party Tool):**\\n    *   Download and run CPU-Z.\\n    *   Go to the \"SPD\" tab. This will show you details for each RAM slot, including the module size, type, and speed.\\n*   **System Information (Windows):**\\n    *   Search for \"System Information\" in the Start Menu.\\n    *   Look for \"Installed Physical Memory (RAM)\" and \"Memory Type.\"\\n\\n#### 2. Check Your Laptop\\'s Manufacturer Website or Manual\\n\\n*   **Most Reliable Method:** Go to the support section of your laptop manufacturer\\'s website (e.g., Dell, HP, Lenovo, Acer, ASUS).\\n*   Enter your exact laptop model number (usually found on a sticker on the bottom of the laptop).\\n*   Look for the \"Specifications\" or \"Manuals\" section. This will definitively tell you:\\n    *   The type of RAM it uses (e.g., DDR4-3200MHz SODIMM).\\n    *   The maximum amount of RAM it supports (e.g., \"Max 16GB RAM\").\\n    *   The number of RAM slots.\\n\\n#### 3. Use an Online RAM Configurator\\n\\n*   Websites like **Crucial.com** or **Kingston.com** have RAM configurators. You can enter your laptop\\'s make and model, and they will tell you exactly what RAM is compatible and how much you can upgrade to. This is highly recommended.\\n\\n#### 4. Physically Inspect Your Laptop (If Comfortable)\\n\\n*   **Only if you\\'re comfortable opening your laptop:**\\n    *   Shut down and unplug your laptop.\\n    *   Remove the battery (if easily removable).\\n    *   Unscrew the bottom panel.\\n    *   Locate the RAM slots. You\\'ll see the RAM sticks clipped into place.\\n    *   The RAM sticks themselves often have labels indicating their capacity, type (e.g., PC4 for DDR4), and speed.\\n\\n---\\n\\n### How to Increase Your RAM (Once You Know the Details):\\n\\nOnce you\\'ve determined compatibility and your laptop\\'s limits, you have a few options:\\n\\n1.  **Add a New RAM Stick (If You Have an Empty Slot):**\\n    *   If you have, say, 8GB of RAM in one slot and an empty second slot, you can buy another 8GB stick of the *exact same type and speed* (or as close as possible) to get a total of 16GB. This is the easiest upgrade.\\n    *   **Important:** For best performance (dual-channel mode), try to match the new stick as closely as possible in capacity, speed, and even brand if you can.\\n\\n2.  **Replace Existing RAM Sticks (If All Slots are Full or You Want More Capacity):**\\n    *   If you have two 4GB sticks (total 8GB) and want 16GB, you\\'d buy two new 8GB sticks of the correct type and speed and replace the old ones.\\n    *   If you have one 8GB stick and want 16GB, and only one slot is available, you\\'d replace the 8GB stick with a 16GB stick.\\n\\n### Step-by-Step Installation (General Guide):\\n\\n**Disclaimer:** If you\\'re not comfortable opening your laptop, consider taking it to a professional computer repair shop.\\n\\n1.  **Purchase Compatible RAM:** Buy the correct type (DDR3, DDR4, DDR5), form factor (SODIMM for laptops), speed (MHz), and capacity (GB) that your laptop supports.\\n2.  **Gather Tools:** Usually just a small Philips head screwdriver. An anti-static wrist strap is recommended.\\n3.  **Power Down & Disconnect:**\\n    *   Shut down your laptop completely (not just sleep).\\n    *   Unplug the power adapter.\\n    *   If your laptop has a removable battery, remove it. If not, you might need to disconnect the internal battery (often requires unscrewing a few more things and being careful).\\n4.  **Open the Laptop:** Carefully unscrew the bottom panel and remove it.\\n5.  **Locate RAM Slots:** They are usually easily identifiable.\\n6.  **Remove Old RAM (If Replacing):** Gently push outwards on the small clips on either side of the RAM stick. The stick will pop up at an angle. You can then pull it straight out.\\n7.  **Install New RAM:**\\n    *   Align the notch on the new RAM stick with the notch in the RAM slot.\\n    *   Insert the stick at a 45-degree angle.\\n    *   Gently push down on the stick until the clips on both sides snap into place, securing it horizontally.\\n8.  **Reassemble:** Put the bottom panel back on and screw it in. Reconnect the battery (if you disconnected it) and plug in the power adapter.\\n9.  **Boot Up and Verify:** Turn on your laptop. It might take a moment longer to boot the first time as it recognizes the new RAM. Once in Windows, check Task Manager (Performance > Memory) or System Information to confirm the new RAM capacity.\\n\\n### What About \"Virtual RAM\" / Page File?\\n\\nYour laptop also uses a \"page file\" or \"swap file\" on your hard drive/SSD as a form of virtual memory. While you can technically increase its size, this is **not a substitute for physical RAM**. Using the page file is *much slower* than using actual RAM, as storage drives are significantly slower than RAM. It\\'s a fallback, not an upgrade for speed.\\n\\nIncreasing your physical RAM is almost always the best way to get a noticeable performance boost for multitasking and demanding applications. Good luck, Tapas!'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response=model_with_memory.invoke([HumanMessage(content=\"How can I increase it?\")],config=config)\n",
    "response.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "2b32db37",
   "metadata": {},
   "outputs": [],
   "source": [
    "config={\"configurable\":{\"session_id\":\"second_chat\"}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e515e7bf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'To give you the best advice, I need to know:\\n\\n**What is \"it\" that you\\'re trying to increase?**\\n\\nOnce I know what \"it\" is, I can provide much more specific and helpful strategies.\\n\\nHowever, here are some general principles that apply to increasing almost anything:\\n\\n1.  **Define \"It\" Clearly:**\\n    *   **Be Specific:** What exactly are you trying to increase? (e.g., income, muscle mass, happiness, knowledge, sales, productivity, a specific number?)\\n    *   **Make it Measurable:** How will you know if it\\'s increasing? What are the metrics? (e.g., from $50k to $70k, from 150 lbs to 160 lbs, from 3 sales calls to 5 sales calls).\\n    *   **Set a Target:** What\\'s your goal? By when do you want to achieve it?\\n\\n2.  **Understand Your Baseline:**\\n    *   Where are you starting from right now? What\\'s the current \"it\"?\\n    *   This helps you track progress and identify areas for improvement.\\n\\n3.  **Analyze the \"Why\":**\\n    *   Why do you want to increase it? What\\'s your motivation? Understanding this can fuel your efforts.\\n\\n4.  **Break It Down:**\\n    *   Once you have a clear goal, break it into smaller, manageable steps.\\n    *   What are the individual actions you need to take consistently?\\n\\n5.  **Identify Levers and Obstacles:**\\n    *   **Levers:** What are the things you can do or change that will directly impact \"it\"? (e.g., for income: new skills, networking, negotiation, side hustle).\\n    *   **Obstacles:** What\\'s currently preventing \"it\" from increasing? What are the challenges you need to overcome?\\n\\n6.  **Learn and Acquire Knowledge:**\\n    *   Read books, take courses, watch tutorials, find mentors, talk to experts who have successfully increased \"it.\"\\n\\n7.  **Formulate a Plan/Strategy:**\\n    *   Based on your research and analysis, create an action plan. What will you do, when will you do it, and how often?\\n\\n8.  **Take Consistent Action:**\\n    *   Execution is key. A plan without action is just a wish. Consistency over time yields results.\\n\\n9.  **Track Progress and Adjust:**\\n    *   Regularly monitor your progress against your baseline and goals.\\n    *   If something isn\\'t working, don\\'t be afraid to adjust your strategy. Learn from what\\'s not working.\\n\\n10. **Stay Persistent and Patient:**\\n    *   Most significant increases take time and effort. There will be setbacks. Resilience is crucial.\\n\\n---\\n\\n**To give you a better idea, here are some common things people want to increase, and general approaches for each:**\\n\\n*   **Income/Money:**\\n    *   **Earn More:** Ask for a raise, get a promotion, find a higher-paying job, start a side hustle, invest in new skills, start a business.\\n    *   **Save More:** Create a budget, cut unnecessary expenses, automate savings.\\n    *   **Invest More:** Learn about investing, diversify your portfolio.\\n*   **Knowledge/Skills:**\\n    *   Read books, take online courses, practice regularly, seek mentorship, attend workshops.\\n*   **Physical Fitness/Muscle Mass:**\\n    *   Follow a structured workout plan (strength training, cardio), optimize nutrition (protein intake), ensure adequate sleep, stay hydrated.\\n*   **Happiness/Well-being:**\\n    *   Practice gratitude, mindfulness/meditation, spend time in nature, nurture relationships, pursue hobbies, help others, limit negative influences.\\n*   **Productivity:**\\n    *   Time management techniques (Pomodoro, time blocking), prioritize tasks (Eisenhower Matrix), minimize distractions, take regular breaks, optimize your workspace.\\n*   **Sales/Business Revenue:**\\n    *   Improve marketing, enhance customer service, optimize pricing, expand product/service offerings, enter new markets, improve sales techniques.\\n\\nPlease tell me what \"it\" is, and I can give you much more tailored and actionable advice!'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response=model_with_memory.invoke([HumanMessage(content=\"How can I increase it?\")],config=config)\n",
    "response.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "25f6cafc",
   "metadata": {},
   "outputs": [],
   "source": [
    "config={\"configurable\":{\"session_id\":\"second_chat\"}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "81910ee1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"As an AI, I don't know your name or any personal information about you. I don't have memory of past conversations or access to your identity.\\n\\nYou would need to tell me your name for me to know it!\""
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response=model_with_memory.invoke([HumanMessage(content=\"What is my name?\")],config=config)\n",
    "response.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "b7330827",
   "metadata": {},
   "outputs": [],
   "source": [
    "config={\"configurable\":{\"session_id\":\"first_chat\"}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "2514d24f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Your name is **Tapas**.'"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response=model_with_memory.invoke([HumanMessage(content=\"What is my name?\")],config=config)\n",
    "response.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "553cce2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate,MessagesPlaceholder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "fc41a2fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt=ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\",\"You are a helpful assistant. Answer all questions to the best of your ability\"),\n",
    "        MessagesPlaceholder(variable_name=\"messages\")\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "6b48dcdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "chain=prompt | llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "3c197655",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"Hello Tapas! It's nice to meet you. How can I help you today?\", additional_kwargs={}, response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'model_name': 'gemini-2.5-flash', 'safety_ratings': []}, id='run--e727bcdc-6ec0-4f2a-a335-e5b38a3dfe0c-0', usage_metadata={'input_tokens': 21, 'output_tokens': 53, 'total_tokens': 74, 'input_token_details': {'cache_read': 0}, 'output_token_details': {'reasoning': 34}})"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.invoke({\"messages\":[\"Hello I am Tapas\"]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "3021986b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"Hello Tapas! It's nice to meet you.\\n\\nHow can I help you today?\", additional_kwargs={}, response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'model_name': 'gemini-2.5-flash', 'safety_ratings': []}, id='run--d147c0cf-245f-4e25-be43-5e1a61bb73e9-0', usage_metadata={'input_tokens': 21, 'output_tokens': 162, 'total_tokens': 183, 'input_token_details': {'cache_read': 0}, 'output_token_details': {'reasoning': 142}})"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.invoke({\"messages\": [HumanMessage(content=\"Hello I am Tapas\")]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "6ce0b5b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_with_memory=RunnableWithMessageHistory(chain,get_session_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "15652a75",
   "metadata": {},
   "outputs": [],
   "source": [
    "config={\"configurable\":{\"session_id\":\"third_chat\"}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "55878bc0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"As an AI, I don't know your name. I don't have access to personal information about you unless you choose to tell me.\", additional_kwargs={}, response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'model_name': 'gemini-2.5-flash', 'safety_ratings': []}, id='run--562f7b03-b057-48ef-a87b-db6338635ffc-0', usage_metadata={'input_tokens': 20, 'output_tokens': 262, 'total_tokens': 282, 'input_token_details': {'cache_read': 0}, 'output_token_details': {'reasoning': 232}})"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_with_memory.invoke({\"messages\": [HumanMessage(content=\"What is my name\")]},config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "61fceae4",
   "metadata": {},
   "outputs": [],
   "source": [
    "config={\"configurable\":{\"session_id\":\"first_chat\"}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "17d70955",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Your name is **Tapas**.', additional_kwargs={}, response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'model_name': 'gemini-2.5-flash', 'safety_ratings': []}, id='run--460992da-d8d8-45b2-9830-8e60f7daa524-0', usage_metadata={'input_tokens': 2459, 'output_tokens': 38, 'total_tokens': 2497, 'input_token_details': {'cache_read': 0}, 'output_token_details': {'reasoning': 31}})"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_with_memory.invoke({\"messages\": [HumanMessage(content=\"What is my name\")]},config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "374b9cb6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Hello Tapas!\\n\\nA **RAG pipeline** stands for **Retrieval-Augmented Generation pipeline**. It\\'s a powerful framework used to enhance the capabilities of Large Language Models (LLMs) like ChatGPT, making them more accurate, up-to-date, and capable of referencing specific, external knowledge.\\n\\n### The Core Problem RAG Solves\\n\\nTraditional LLMs are trained on a vast amount of data up to a certain point in time. This means they:\\n1.  Have a **knowledge cutoff** (they don\\'t know about recent events).\\n2.  Can **\"hallucinate\"** or confidently make up facts if they don\\'t have the information in their training data.\\n3.  Lack access to **private, domain-specific, or real-time information** (e.g., your company\\'s internal documents, today\\'s news headlines).\\n\\nRAG addresses these limitations by giving the LLM an \"open book\" to consult before generating a response.\\n\\n### The Analogy\\n\\nImagine you\\'re a student taking an exam.\\n*   A regular LLM is like a student who has studied a lot and tries to answer questions based *only* on what they remember. They might forget details or even make things up if they\\'re unsure.\\n*   A RAG-powered LLM is like that same student, but *with access to a textbook or notes* during the exam. When asked a question, they first *look up* the relevant information in their notes (retrieval) and then *formulate an answer* based on what they found (generation).\\n\\n### How a RAG Pipeline Works (The Steps)\\n\\nA RAG pipeline typically involves two main phases: **Retrieval** and **Generation**.\\n\\n#### Phase 1: Retrieval (Finding Relevant Information)\\n\\nThis phase is about finding the most relevant pieces of information from a knowledge base *outside* the LLM\\'s core training data.\\n\\n1.  **Data Ingestion & Indexing:**\\n    *   **Source Data:** You start with your external knowledge base. This could be documents (PDFs, Word docs), websites, databases, articles, etc.\\n    *   **Chunking:** This data is broken down into smaller, manageable \"chunks\" or segments. This is important because LLMs have a limited \"context window\" (how much text they can process at once).\\n    *   **Embedding:** Each chunk is then converted into a numerical representation called an **embedding** using an **embedding model**. Embeddings capture the semantic meaning of the text, so similar chunks of text will have similar embeddings.\\n    *   **Vector Database Storage:** These embeddings (along with a reference back to the original text chunk) are stored in a specialized database called a **vector database** (or vector store). This database is optimized for very fast similarity searches.\\n\\n2.  **Query Processing & Search:**\\n    *   **User Query:** When a user asks a question, their query is also converted into an embedding using the same embedding model.\\n    *   **Similarity Search:** The system then performs a similarity search in the vector database. It compares the user\\'s query embedding to all the stored chunk embeddings to find the most semantically similar chunks.\\n    *   **Retrieval:** The top N most relevant text chunks are retrieved.\\n\\n#### Phase 2: Generation (Formulating the Answer)\\n\\nThis phase takes the retrieved information and uses it to inform the LLM\\'s response.\\n\\n1.  **Prompt Augmentation:**\\n    *   The user\\'s original query and the retrieved relevant text chunks are combined into a single, comprehensive **prompt**.\\n    *   The prompt typically instructs the LLM to answer the question *using only the provided context* to prevent it from hallucinating or relying on its general training data.\\n    *   Example: \"Based on the following context, please answer the question: [Retrieved Context 1] [Retrieved Context 2]... Question: [User\\'s Original Query]\"\\n\\n2.  **LLM Inference:**\\n    *   This augmented prompt is then fed into the Large Language Model.\\n    *   The LLM processes this prompt, using the provided context as its primary source of truth.\\n\\n3.  **Response Generation:**\\n    *   The LLM generates a coherent, accurate, and contextually relevant answer, grounded in the retrieved information.\\n\\n### Benefits of RAG\\n\\n*   **Improved Accuracy & Reduced Hallucinations:** Answers are based on verifiable facts from the knowledge base.\\n*   **Up-to-Date Information:** Can access the latest data by simply updating the knowledge base, without retraining the LLM.\\n*   **Domain-Specific Knowledge:** Enables LLMs to answer questions about specific, private, or proprietary information (e.g., internal company policies).\\n*   **Transparency & Trust:** Can often provide citations or source links to the retrieved documents, allowing users to verify information.\\n*   **Cost-Effective:** Avoids the need for expensive and time-consuming LLM fine-tuning or retraining for new data.\\n*   **Smaller LLMs:** Can achieve high performance with smaller, more efficient LLMs because they don\\'t need to \"memorize\" everything.\\n\\n### Common Use Cases\\n\\n*   **Customer Support Chatbots:** Answering customer queries based on product manuals, FAQs, and support articles.\\n*   **Enterprise Search:** Allowing employees to query internal documents, reports, and knowledge bases.\\n*   **Legal Research:** Providing answers from legal texts, case law, and statutes.\\n*   **Medical Information Systems:** Querying medical journals and patient records.\\n*   **Personalized Content Generation:** Generating summaries or recommendations based on a user\\'s specific data.\\n\\nIn essence, a RAG pipeline transforms a general-purpose LLM into a highly informed, specialized, and reliable expert by giving it the ability to \"look things up\" in real-time.', additional_kwargs={}, response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'model_name': 'gemini-2.5-flash', 'safety_ratings': []}, id='run--7aef0514-3652-45d9-9d45-3a8d613acf72-0', usage_metadata={'input_tokens': 2473, 'output_tokens': 2332, 'total_tokens': 4805, 'input_token_details': {'cache_read': 1993}, 'output_token_details': {'reasoning': 1085}})"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_with_memory.invoke({\"messages\": [HumanMessage(content=\"What is rag pipeline?\")]},config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "c97c89bb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='That\\'s an excellent follow-up question, Tapas! Storing data for a RAG pipeline involves two main types of storage, each serving a distinct purpose:\\n\\n1.  **Storage for the Raw, Original Data:** This is where your actual documents, texts, or information reside.\\n2.  **Storage for the Embeddings (Vector Database):** This is the specialized storage for the numerical representations of your data chunks, optimized for fast similarity searches.\\n\\nLet\\'s break down how you store data for each part:\\n\\n---\\n\\n### 1. Storing the Raw, Original Data (Your Knowledge Base)\\n\\nThis is where your source material lives before it\\'s processed. The choice depends on the nature and scale of your data.\\n\\n**What to store:**\\n*   Full text of documents (PDFs, Word docs, HTML, Markdown, text files)\\n*   Database records\\n*   Web pages\\n*   Transcripts of audio/video\\n*   Internal company wikis, manuals, reports\\n\\n**Where to store it:**\\n\\n*   **Local Filesystem:**\\n    *   **Pros:** Simplest for small, personal projects or testing. Easy to access.\\n    *   **Cons:** Not scalable, not robust for production, difficult to share or manage centrally.\\n    *   **Use case:** A few PDF documents on your laptop for a local RAG app.\\n\\n*   **Cloud Object Storage (e.g., AWS S3, Google Cloud Storage, Azure Blob Storage):**\\n    *   **Pros:** Highly scalable, durable, cost-effective, easy to integrate with cloud services, good for unstructured data.\\n    *   **Cons:** Can incur egress costs for large data transfers.\\n    *   **Use case:** Storing millions of company documents, web archives, or large datasets that need to be accessed by a RAG pipeline running in the cloud.\\n\\n*   **Traditional Databases (SQL or NoSQL):**\\n    *   **Pros:** Good for structured or semi-structured data, offers transactional integrity, robust querying capabilities beyond just text.\\n    *   **Cons:** Can be less efficient for purely unstructured text, might require more effort to extract text for chunking.\\n    *   **Use case:** If your knowledge base is primarily a set of database records (e.g., product descriptions, customer support tickets stored in a database), you\\'d pull the relevant text from here.\\n\\n*   **Content Management Systems (CMS) or Document Management Systems (DMS):**\\n    *   **Pros:** Designed for managing documents, often have version control, access control, and metadata features built-in.\\n    *   **Cons:** Can be more complex to integrate directly for raw text extraction.\\n    *   **Use case:** If your company already uses SharePoint, Confluence, or similar, you\\'d integrate with these to pull documents.\\n\\n*   **Data Lakes/Warehouses:**\\n    *   **Pros:** For extremely large, diverse datasets, often used for analytics but can serve as a source for RAG.\\n    *   **Cons:** Overkill for many RAG projects, requires robust ETL pipelines.\\n    *   **Use case:** Enterprise-level RAG systems that draw from an organization\\'s entire data ecosystem.\\n\\n---\\n\\n### 2. Storing the Embeddings (The Vector Database/Store)\\n\\nThis is the most critical component for the \"Retrieval\" part of RAG. It stores the numerical vectors and allows for efficient similarity searches.\\n\\n**What to store:**\\n*   **Vector Embeddings:** The high-dimensional numerical representation of each text chunk.\\n*   **Original Text Chunk:** The actual text segment that the vector represents. (This is essential because the LLM needs the *text* to generate an answer, not just the vector).\\n*   **Metadata:** Additional information about the chunk (e.g., source document, page number, author, date, keywords). This metadata is crucial for filtering results (e.g., \"only show me results from documents published after 2022\").\\n\\n**Where to store it (Vector Databases/Stores):**\\n\\nVector databases are purpose-built for storing and querying high-dimensional vectors. They use specialized indexing algorithms (like HNSW, IVF, LSH) to perform fast Approximate Nearest Neighbor (ANN) searches.\\n\\n*   **Dedicated Vector Databases (Managed Services):**\\n    *   **Examples:** Pinecone, Weaviate, Qdrant, Milvus, Zilliz Cloud.\\n    *   **Pros:** Highly scalable, optimized for performance, robust features, often provide a managed service (less operational overhead).\\n    *   **Cons:** Can be more expensive, might require learning a new API.\\n    *   **Use case:** Production-grade RAG applications, large-scale knowledge bases, complex filtering requirements.\\n\\n*   **Open-Source Vector Libraries/Databases:**\\n    *   **Examples:** Chroma, FAISS (Facebook AI Similarity Search - a library, not a full DB), LanceDB.\\n    *   **Pros:** Free, flexible, can be self-hosted, good for prototyping or smaller projects.\\n    *   **Cons:** Requires more setup and management, might not scale as easily as managed services.\\n    *   **Use case:** Local development, small to medium-sized applications, when you need fine-grained control over the infrastructure.\\n\\n*   **Vector Extensions for Traditional Databases:**\\n    *   **Examples:** `pgvector` for PostgreSQL, `Redis Stack` (with RediSearch).\\n    *   **Pros:** Leverages existing database infrastructure, combines relational data with vector search, good for hybrid data models.\\n    *   **Cons:** Might not offer the same raw performance or scalability as dedicated vector databases for very large vector sets.\\n    *   **Use case:** When you already have a strong PostgreSQL or Redis setup and want to add vector search capabilities without introducing a new database system.\\n\\n*   **In-Memory (for small-scale testing):**\\n    *   **Examples:** Simple Python lists/dictionaries, or libraries like `Annoy` for very small, local tests.\\n    *   **Pros:** Extremely fast for tiny datasets, easy to implement.\\n    *   **Cons:** Not persistent, not scalable, only for very small proofs-of-concept.\\n    *   **Use case:** Quick local experiments or learning.\\n\\n---\\n\\n### The Data Flow for Storage\\n\\n1.  **Ingestion:** You read your raw data from its source (e.g., files, cloud storage, database).\\n2.  **Preprocessing:** Clean the text (remove unnecessary formatting, headers, footers).\\n3.  **Chunking:** Break the text into smaller, meaningful segments (e.g., 200-500 words with some overlap). This is crucial for matching specific parts of your document to the query.\\n4.  **Embedding:** For each chunk, use an **embedding model** (e.g., OpenAI\\'s `text-embedding-ada-002`, Sentence Transformers) to convert the text into a numerical vector.\\n5.  **Storage in Vector Database:** Store the generated vector, the original text chunk, and any relevant metadata into your chosen vector database.\\n\\nBy using this two-tiered storage approach, your RAG pipeline can efficiently retrieve relevant information and ensure the LLM has the precise context it needs to generate accurate answers.', additional_kwargs={}, response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'model_name': 'gemini-2.5-flash', 'safety_ratings': []}, id='run--7e98cd7b-90e6-4c29-a1a2-2278e3f63448-0', usage_metadata={'input_tokens': 3730, 'output_tokens': 2565, 'total_tokens': 6295, 'input_token_details': {'cache_read': 2007}, 'output_token_details': {'reasoning': 996}})"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_with_memory.invoke({\"messages\": [HumanMessage(content=\"How can I store data for this?\")]},config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "c8a24a37",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'first_chat': InMemoryChatMessageHistory(messages=[HumanMessage(content='Hello My name is Tapas', additional_kwargs={}, response_metadata={}), AIMessage(content=\"Hello Tapas! It's nice to meet you.\", additional_kwargs={}, response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'model_name': 'gemini-2.5-flash', 'safety_ratings': []}, id='run--03e0ed11-d071-4749-b4a9-e9574748262f-0', usage_metadata={'input_tokens': 7, 'output_tokens': 319, 'total_tokens': 326, 'input_token_details': {'cache_read': 0}, 'output_token_details': {'reasoning': 307}}), HumanMessage(content='What is ram in laptop?', additional_kwargs={}, response_metadata={}), AIMessage(content='Hello Tapas! That\\'s a great question, as RAM is one of the most crucial components in any laptop.\\n\\n**RAM** stands for **R**andom **A**ccess **M**emory.\\n\\nThink of your laptop like this:\\n\\n*   **Your Hard Drive (HDD) or Solid State Drive (SSD)** is like a **filing cabinet** or a **library**. It\\'s where all your programs, documents, photos, and the operating system are permanently stored. It\\'s vast, but accessing things from it takes time.\\n*   **Your Laptop\\'s Processor (CPU)** is like your **brain**. It does all the thinking and calculations.\\n*   **RAM** is like your **desk or workbench**. When you decide to work on something (open a program, edit a document, browse the web), your laptop pulls the necessary data from the slow \"filing cabinet\" (storage) and places it onto the super-fast \"desk\" (RAM).\\n\\n**Here\\'s a breakdown of what RAM is and what it does:**\\n\\n1.  **Temporary, High-Speed Storage:** RAM is a very fast type of memory that your laptop uses to store data that it\\'s actively working with *right now*. Unlike your hard drive, it\\'s not for long-term storage.\\n2.  **The \"Working Space\":** When you open an application (like a web browser, a word processor, or a game), the laptop loads parts of that application into RAM. This allows the CPU to access that data almost instantly, making your laptop feel responsive.\\n3.  **Multitasking Power:** The more RAM you have, the more \"workspace\" your laptop has. This means you can have more applications open simultaneously, switch between them smoothly, and run demanding programs without your laptop slowing down.\\n4.  **Volatile Memory:** This is a key characteristic. When you turn off your laptop, everything stored in RAM is erased. That\\'s why you need to save your work to your hard drive or SSD before shutting down!\\n\\n**Why is RAM important?**\\n\\n*   **Speed:** RAM is significantly faster than your hard drive or SSD. If your laptop had to constantly pull data directly from the storage drive, it would be incredibly slow.\\n*   **Responsiveness:** Adequate RAM ensures that your applications load quickly, tabs in your browser don\\'t constantly reload, and your overall computing experience is smooth.\\n*   **Multitasking:** If you\\'re someone who likes to have many browser tabs open, several applications running, or you edit videos/photos, more RAM is essential.\\n*   **Gaming:** Games are very demanding on RAM as they need to store a lot of textures, levels, and game data for quick access.\\n\\n**How much RAM do you need?**\\n\\n*   **4GB:** Very basic use (light browsing, email, simple documents). Becoming less common and often too little for a good experience.\\n*   **8GB:** Good for most everyday users (web browsing, office work, streaming, light photo editing). This is a common sweet spot.\\n*   **16GB:** Excellent for power users, gamers, content creators (video editing, graphic design), and those who run many applications simultaneously.\\n*   **32GB+:** For professional-grade work, heavy gaming, virtual machines, and specialized scientific or engineering applications.\\n\\nIn summary, RAM is your laptop\\'s short-term, super-fast memory that allows it to efficiently run programs and manage multiple tasks. It\\'s crucial for a smooth and responsive computing experience!', additional_kwargs={}, response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'model_name': 'gemini-2.5-flash', 'safety_ratings': []}, id='run--a83fd19e-202a-4a1c-9f38-9bfd1c654b11-0', usage_metadata={'input_tokens': 27, 'output_tokens': 1643, 'total_tokens': 1670, 'input_token_details': {'cache_read': 0}, 'output_token_details': {'reasoning': 889}}), HumanMessage(content='How can I increase it?', additional_kwargs={}, response_metadata={}), AIMessage(content='Yes, Tapas, you can often increase the RAM in a laptop, but it depends on your specific laptop model. It\\'s one of the most common and effective upgrades you can make to improve performance, especially for multitasking and demanding applications.\\n\\nHere\\'s how you can go about it:\\n\\n### Crucial First Steps: Determine if your RAM is Upgradeable\\n\\nBefore you buy anything, you need to figure out a few things:\\n\\n1.  **Is the RAM Soldered?** Some modern, ultra-slim laptops (especially ultrabooks, MacBooks, and some budget models) have their RAM soldered directly onto the motherboard. If this is the case, it\\'s **not upgradeable** at all.\\n2.  **How Many RAM Slots Do You Have?** Most laptops have either one or two RAM slots.\\n3.  **What Type of RAM Does Your Laptop Use?** (DDR3, DDR4, DDR5) and what is its speed (MHz)?\\n4.  **What is the Maximum RAM Your Laptop Supports?** Every motherboard has a limit.\\n5.  **What RAM is Currently Installed?** (Capacity per stick, e.g., 4GB, 8GB).\\n\\n### How to Find This Information:\\n\\n#### 1. Software Tools (Easiest Way)\\n\\n*   **Windows Task Manager (Windows 10/11):**\\n    *   Right-click on the Taskbar and select \"Task Manager\" (or press Ctrl+Shift+Esc).\\n    *   Go to the \"Performance\" tab.\\n    *   Click on \"Memory.\"\\n    *   Here you\\'ll see the total RAM, speed, form factor (SODIMM), and how many slots are \"Used\" out of the total available. This will tell you if you have an empty slot.\\n*   **CPU-Z (Free Third-Party Tool):**\\n    *   Download and run CPU-Z.\\n    *   Go to the \"SPD\" tab. This will show you details for each RAM slot, including the module size, type, and speed.\\n*   **System Information (Windows):**\\n    *   Search for \"System Information\" in the Start Menu.\\n    *   Look for \"Installed Physical Memory (RAM)\" and \"Memory Type.\"\\n\\n#### 2. Check Your Laptop\\'s Manufacturer Website or Manual\\n\\n*   **Most Reliable Method:** Go to the support section of your laptop manufacturer\\'s website (e.g., Dell, HP, Lenovo, Acer, ASUS).\\n*   Enter your exact laptop model number (usually found on a sticker on the bottom of the laptop).\\n*   Look for the \"Specifications\" or \"Manuals\" section. This will definitively tell you:\\n    *   The type of RAM it uses (e.g., DDR4-3200MHz SODIMM).\\n    *   The maximum amount of RAM it supports (e.g., \"Max 16GB RAM\").\\n    *   The number of RAM slots.\\n\\n#### 3. Use an Online RAM Configurator\\n\\n*   Websites like **Crucial.com** or **Kingston.com** have RAM configurators. You can enter your laptop\\'s make and model, and they will tell you exactly what RAM is compatible and how much you can upgrade to. This is highly recommended.\\n\\n#### 4. Physically Inspect Your Laptop (If Comfortable)\\n\\n*   **Only if you\\'re comfortable opening your laptop:**\\n    *   Shut down and unplug your laptop.\\n    *   Remove the battery (if easily removable).\\n    *   Unscrew the bottom panel.\\n    *   Locate the RAM slots. You\\'ll see the RAM sticks clipped into place.\\n    *   The RAM sticks themselves often have labels indicating their capacity, type (e.g., PC4 for DDR4), and speed.\\n\\n---\\n\\n### How to Increase Your RAM (Once You Know the Details):\\n\\nOnce you\\'ve determined compatibility and your laptop\\'s limits, you have a few options:\\n\\n1.  **Add a New RAM Stick (If You Have an Empty Slot):**\\n    *   If you have, say, 8GB of RAM in one slot and an empty second slot, you can buy another 8GB stick of the *exact same type and speed* (or as close as possible) to get a total of 16GB. This is the easiest upgrade.\\n    *   **Important:** For best performance (dual-channel mode), try to match the new stick as closely as possible in capacity, speed, and even brand if you can.\\n\\n2.  **Replace Existing RAM Sticks (If All Slots are Full or You Want More Capacity):**\\n    *   If you have two 4GB sticks (total 8GB) and want 16GB, you\\'d buy two new 8GB sticks of the correct type and speed and replace the old ones.\\n    *   If you have one 8GB stick and want 16GB, and only one slot is available, you\\'d replace the 8GB stick with a 16GB stick.\\n\\n### Step-by-Step Installation (General Guide):\\n\\n**Disclaimer:** If you\\'re not comfortable opening your laptop, consider taking it to a professional computer repair shop.\\n\\n1.  **Purchase Compatible RAM:** Buy the correct type (DDR3, DDR4, DDR5), form factor (SODIMM for laptops), speed (MHz), and capacity (GB) that your laptop supports.\\n2.  **Gather Tools:** Usually just a small Philips head screwdriver. An anti-static wrist strap is recommended.\\n3.  **Power Down & Disconnect:**\\n    *   Shut down your laptop completely (not just sleep).\\n    *   Unplug the power adapter.\\n    *   If your laptop has a removable battery, remove it. If not, you might need to disconnect the internal battery (often requires unscrewing a few more things and being careful).\\n4.  **Open the Laptop:** Carefully unscrew the bottom panel and remove it.\\n5.  **Locate RAM Slots:** They are usually easily identifiable.\\n6.  **Remove Old RAM (If Replacing):** Gently push outwards on the small clips on either side of the RAM stick. The stick will pop up at an angle. You can then pull it straight out.\\n7.  **Install New RAM:**\\n    *   Align the notch on the new RAM stick with the notch in the RAM slot.\\n    *   Insert the stick at a 45-degree angle.\\n    *   Gently push down on the stick until the clips on both sides snap into place, securing it horizontally.\\n8.  **Reassemble:** Put the bottom panel back on and screw it in. Reconnect the battery (if you disconnected it) and plug in the power adapter.\\n9.  **Boot Up and Verify:** Turn on your laptop. It might take a moment longer to boot the first time as it recognizes the new RAM. Once in Windows, check Task Manager (Performance > Memory) or System Information to confirm the new RAM capacity.\\n\\n### What About \"Virtual RAM\" / Page File?\\n\\nYour laptop also uses a \"page file\" or \"swap file\" on your hard drive/SSD as a form of virtual memory. While you can technically increase its size, this is **not a substitute for physical RAM**. Using the page file is *much slower* than using actual RAM, as storage drives are significantly slower than RAM. It\\'s a fallback, not an upgrade for speed.\\n\\nIncreasing your physical RAM is almost always the best way to get a noticeable performance boost for multitasking and demanding applications. Good luck, Tapas!', additional_kwargs={}, response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'model_name': 'gemini-2.5-flash', 'safety_ratings': []}, id='run--5e03a71f-aa30-41c2-a9f3-a73e019b095a-0', usage_metadata={'input_tokens': 789, 'output_tokens': 2789, 'total_tokens': 3578, 'input_token_details': {'cache_read': 0}, 'output_token_details': {'reasoning': 1154}}), HumanMessage(content='What is my name?', additional_kwargs={}, response_metadata={}), AIMessage(content='Your name is **Tapas**.', additional_kwargs={}, response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'model_name': 'gemini-2.5-flash', 'safety_ratings': []}, id='run--c9b4761f-d5e7-4d93-b117-9c2554e13cc0-0', usage_metadata={'input_tokens': 2431, 'output_tokens': 43, 'total_tokens': 2474, 'input_token_details': {'cache_read': 0}, 'output_token_details': {'reasoning': 36}}), HumanMessage(content='What is my name', additional_kwargs={}, response_metadata={}), AIMessage(content='Your name is **Tapas**.', additional_kwargs={}, response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'model_name': 'gemini-2.5-flash', 'safety_ratings': []}, id='run--460992da-d8d8-45b2-9830-8e60f7daa524-0', usage_metadata={'input_tokens': 2459, 'output_tokens': 38, 'total_tokens': 2497, 'input_token_details': {'cache_read': 0}, 'output_token_details': {'reasoning': 31}}), HumanMessage(content='What is rag pipeline?', additional_kwargs={}, response_metadata={}), AIMessage(content='Hello Tapas!\\n\\nA **RAG pipeline** stands for **Retrieval-Augmented Generation pipeline**. It\\'s a powerful framework used to enhance the capabilities of Large Language Models (LLMs) like ChatGPT, making them more accurate, up-to-date, and capable of referencing specific, external knowledge.\\n\\n### The Core Problem RAG Solves\\n\\nTraditional LLMs are trained on a vast amount of data up to a certain point in time. This means they:\\n1.  Have a **knowledge cutoff** (they don\\'t know about recent events).\\n2.  Can **\"hallucinate\"** or confidently make up facts if they don\\'t have the information in their training data.\\n3.  Lack access to **private, domain-specific, or real-time information** (e.g., your company\\'s internal documents, today\\'s news headlines).\\n\\nRAG addresses these limitations by giving the LLM an \"open book\" to consult before generating a response.\\n\\n### The Analogy\\n\\nImagine you\\'re a student taking an exam.\\n*   A regular LLM is like a student who has studied a lot and tries to answer questions based *only* on what they remember. They might forget details or even make things up if they\\'re unsure.\\n*   A RAG-powered LLM is like that same student, but *with access to a textbook or notes* during the exam. When asked a question, they first *look up* the relevant information in their notes (retrieval) and then *formulate an answer* based on what they found (generation).\\n\\n### How a RAG Pipeline Works (The Steps)\\n\\nA RAG pipeline typically involves two main phases: **Retrieval** and **Generation**.\\n\\n#### Phase 1: Retrieval (Finding Relevant Information)\\n\\nThis phase is about finding the most relevant pieces of information from a knowledge base *outside* the LLM\\'s core training data.\\n\\n1.  **Data Ingestion & Indexing:**\\n    *   **Source Data:** You start with your external knowledge base. This could be documents (PDFs, Word docs), websites, databases, articles, etc.\\n    *   **Chunking:** This data is broken down into smaller, manageable \"chunks\" or segments. This is important because LLMs have a limited \"context window\" (how much text they can process at once).\\n    *   **Embedding:** Each chunk is then converted into a numerical representation called an **embedding** using an **embedding model**. Embeddings capture the semantic meaning of the text, so similar chunks of text will have similar embeddings.\\n    *   **Vector Database Storage:** These embeddings (along with a reference back to the original text chunk) are stored in a specialized database called a **vector database** (or vector store). This database is optimized for very fast similarity searches.\\n\\n2.  **Query Processing & Search:**\\n    *   **User Query:** When a user asks a question, their query is also converted into an embedding using the same embedding model.\\n    *   **Similarity Search:** The system then performs a similarity search in the vector database. It compares the user\\'s query embedding to all the stored chunk embeddings to find the most semantically similar chunks.\\n    *   **Retrieval:** The top N most relevant text chunks are retrieved.\\n\\n#### Phase 2: Generation (Formulating the Answer)\\n\\nThis phase takes the retrieved information and uses it to inform the LLM\\'s response.\\n\\n1.  **Prompt Augmentation:**\\n    *   The user\\'s original query and the retrieved relevant text chunks are combined into a single, comprehensive **prompt**.\\n    *   The prompt typically instructs the LLM to answer the question *using only the provided context* to prevent it from hallucinating or relying on its general training data.\\n    *   Example: \"Based on the following context, please answer the question: [Retrieved Context 1] [Retrieved Context 2]... Question: [User\\'s Original Query]\"\\n\\n2.  **LLM Inference:**\\n    *   This augmented prompt is then fed into the Large Language Model.\\n    *   The LLM processes this prompt, using the provided context as its primary source of truth.\\n\\n3.  **Response Generation:**\\n    *   The LLM generates a coherent, accurate, and contextually relevant answer, grounded in the retrieved information.\\n\\n### Benefits of RAG\\n\\n*   **Improved Accuracy & Reduced Hallucinations:** Answers are based on verifiable facts from the knowledge base.\\n*   **Up-to-Date Information:** Can access the latest data by simply updating the knowledge base, without retraining the LLM.\\n*   **Domain-Specific Knowledge:** Enables LLMs to answer questions about specific, private, or proprietary information (e.g., internal company policies).\\n*   **Transparency & Trust:** Can often provide citations or source links to the retrieved documents, allowing users to verify information.\\n*   **Cost-Effective:** Avoids the need for expensive and time-consuming LLM fine-tuning or retraining for new data.\\n*   **Smaller LLMs:** Can achieve high performance with smaller, more efficient LLMs because they don\\'t need to \"memorize\" everything.\\n\\n### Common Use Cases\\n\\n*   **Customer Support Chatbots:** Answering customer queries based on product manuals, FAQs, and support articles.\\n*   **Enterprise Search:** Allowing employees to query internal documents, reports, and knowledge bases.\\n*   **Legal Research:** Providing answers from legal texts, case law, and statutes.\\n*   **Medical Information Systems:** Querying medical journals and patient records.\\n*   **Personalized Content Generation:** Generating summaries or recommendations based on a user\\'s specific data.\\n\\nIn essence, a RAG pipeline transforms a general-purpose LLM into a highly informed, specialized, and reliable expert by giving it the ability to \"look things up\" in real-time.', additional_kwargs={}, response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'model_name': 'gemini-2.5-flash', 'safety_ratings': []}, id='run--7aef0514-3652-45d9-9d45-3a8d613acf72-0', usage_metadata={'input_tokens': 2473, 'output_tokens': 2332, 'total_tokens': 4805, 'input_token_details': {'cache_read': 1993}, 'output_token_details': {'reasoning': 1085}}), HumanMessage(content='How can I store data for this?', additional_kwargs={}, response_metadata={}), AIMessage(content='That\\'s an excellent follow-up question, Tapas! Storing data for a RAG pipeline involves two main types of storage, each serving a distinct purpose:\\n\\n1.  **Storage for the Raw, Original Data:** This is where your actual documents, texts, or information reside.\\n2.  **Storage for the Embeddings (Vector Database):** This is the specialized storage for the numerical representations of your data chunks, optimized for fast similarity searches.\\n\\nLet\\'s break down how you store data for each part:\\n\\n---\\n\\n### 1. Storing the Raw, Original Data (Your Knowledge Base)\\n\\nThis is where your source material lives before it\\'s processed. The choice depends on the nature and scale of your data.\\n\\n**What to store:**\\n*   Full text of documents (PDFs, Word docs, HTML, Markdown, text files)\\n*   Database records\\n*   Web pages\\n*   Transcripts of audio/video\\n*   Internal company wikis, manuals, reports\\n\\n**Where to store it:**\\n\\n*   **Local Filesystem:**\\n    *   **Pros:** Simplest for small, personal projects or testing. Easy to access.\\n    *   **Cons:** Not scalable, not robust for production, difficult to share or manage centrally.\\n    *   **Use case:** A few PDF documents on your laptop for a local RAG app.\\n\\n*   **Cloud Object Storage (e.g., AWS S3, Google Cloud Storage, Azure Blob Storage):**\\n    *   **Pros:** Highly scalable, durable, cost-effective, easy to integrate with cloud services, good for unstructured data.\\n    *   **Cons:** Can incur egress costs for large data transfers.\\n    *   **Use case:** Storing millions of company documents, web archives, or large datasets that need to be accessed by a RAG pipeline running in the cloud.\\n\\n*   **Traditional Databases (SQL or NoSQL):**\\n    *   **Pros:** Good for structured or semi-structured data, offers transactional integrity, robust querying capabilities beyond just text.\\n    *   **Cons:** Can be less efficient for purely unstructured text, might require more effort to extract text for chunking.\\n    *   **Use case:** If your knowledge base is primarily a set of database records (e.g., product descriptions, customer support tickets stored in a database), you\\'d pull the relevant text from here.\\n\\n*   **Content Management Systems (CMS) or Document Management Systems (DMS):**\\n    *   **Pros:** Designed for managing documents, often have version control, access control, and metadata features built-in.\\n    *   **Cons:** Can be more complex to integrate directly for raw text extraction.\\n    *   **Use case:** If your company already uses SharePoint, Confluence, or similar, you\\'d integrate with these to pull documents.\\n\\n*   **Data Lakes/Warehouses:**\\n    *   **Pros:** For extremely large, diverse datasets, often used for analytics but can serve as a source for RAG.\\n    *   **Cons:** Overkill for many RAG projects, requires robust ETL pipelines.\\n    *   **Use case:** Enterprise-level RAG systems that draw from an organization\\'s entire data ecosystem.\\n\\n---\\n\\n### 2. Storing the Embeddings (The Vector Database/Store)\\n\\nThis is the most critical component for the \"Retrieval\" part of RAG. It stores the numerical vectors and allows for efficient similarity searches.\\n\\n**What to store:**\\n*   **Vector Embeddings:** The high-dimensional numerical representation of each text chunk.\\n*   **Original Text Chunk:** The actual text segment that the vector represents. (This is essential because the LLM needs the *text* to generate an answer, not just the vector).\\n*   **Metadata:** Additional information about the chunk (e.g., source document, page number, author, date, keywords). This metadata is crucial for filtering results (e.g., \"only show me results from documents published after 2022\").\\n\\n**Where to store it (Vector Databases/Stores):**\\n\\nVector databases are purpose-built for storing and querying high-dimensional vectors. They use specialized indexing algorithms (like HNSW, IVF, LSH) to perform fast Approximate Nearest Neighbor (ANN) searches.\\n\\n*   **Dedicated Vector Databases (Managed Services):**\\n    *   **Examples:** Pinecone, Weaviate, Qdrant, Milvus, Zilliz Cloud.\\n    *   **Pros:** Highly scalable, optimized for performance, robust features, often provide a managed service (less operational overhead).\\n    *   **Cons:** Can be more expensive, might require learning a new API.\\n    *   **Use case:** Production-grade RAG applications, large-scale knowledge bases, complex filtering requirements.\\n\\n*   **Open-Source Vector Libraries/Databases:**\\n    *   **Examples:** Chroma, FAISS (Facebook AI Similarity Search - a library, not a full DB), LanceDB.\\n    *   **Pros:** Free, flexible, can be self-hosted, good for prototyping or smaller projects.\\n    *   **Cons:** Requires more setup and management, might not scale as easily as managed services.\\n    *   **Use case:** Local development, small to medium-sized applications, when you need fine-grained control over the infrastructure.\\n\\n*   **Vector Extensions for Traditional Databases:**\\n    *   **Examples:** `pgvector` for PostgreSQL, `Redis Stack` (with RediSearch).\\n    *   **Pros:** Leverages existing database infrastructure, combines relational data with vector search, good for hybrid data models.\\n    *   **Cons:** Might not offer the same raw performance or scalability as dedicated vector databases for very large vector sets.\\n    *   **Use case:** When you already have a strong PostgreSQL or Redis setup and want to add vector search capabilities without introducing a new database system.\\n\\n*   **In-Memory (for small-scale testing):**\\n    *   **Examples:** Simple Python lists/dictionaries, or libraries like `Annoy` for very small, local tests.\\n    *   **Pros:** Extremely fast for tiny datasets, easy to implement.\\n    *   **Cons:** Not persistent, not scalable, only for very small proofs-of-concept.\\n    *   **Use case:** Quick local experiments or learning.\\n\\n---\\n\\n### The Data Flow for Storage\\n\\n1.  **Ingestion:** You read your raw data from its source (e.g., files, cloud storage, database).\\n2.  **Preprocessing:** Clean the text (remove unnecessary formatting, headers, footers).\\n3.  **Chunking:** Break the text into smaller, meaningful segments (e.g., 200-500 words with some overlap). This is crucial for matching specific parts of your document to the query.\\n4.  **Embedding:** For each chunk, use an **embedding model** (e.g., OpenAI\\'s `text-embedding-ada-002`, Sentence Transformers) to convert the text into a numerical vector.\\n5.  **Storage in Vector Database:** Store the generated vector, the original text chunk, and any relevant metadata into your chosen vector database.\\n\\nBy using this two-tiered storage approach, your RAG pipeline can efficiently retrieve relevant information and ensure the LLM has the precise context it needs to generate accurate answers.', additional_kwargs={}, response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'model_name': 'gemini-2.5-flash', 'safety_ratings': []}, id='run--7e98cd7b-90e6-4c29-a1a2-2278e3f63448-0', usage_metadata={'input_tokens': 3730, 'output_tokens': 2565, 'total_tokens': 6295, 'input_token_details': {'cache_read': 2007}, 'output_token_details': {'reasoning': 996}})]),\n",
       " 'second_chat': InMemoryChatMessageHistory(messages=[HumanMessage(content='How can I increase it?', additional_kwargs={}, response_metadata={}), AIMessage(content='To give you the best advice, I need to know:\\n\\n**What is \"it\" that you\\'re trying to increase?**\\n\\nOnce I know what \"it\" is, I can provide much more specific and helpful strategies.\\n\\nHowever, here are some general principles that apply to increasing almost anything:\\n\\n1.  **Define \"It\" Clearly:**\\n    *   **Be Specific:** What exactly are you trying to increase? (e.g., income, muscle mass, happiness, knowledge, sales, productivity, a specific number?)\\n    *   **Make it Measurable:** How will you know if it\\'s increasing? What are the metrics? (e.g., from $50k to $70k, from 150 lbs to 160 lbs, from 3 sales calls to 5 sales calls).\\n    *   **Set a Target:** What\\'s your goal? By when do you want to achieve it?\\n\\n2.  **Understand Your Baseline:**\\n    *   Where are you starting from right now? What\\'s the current \"it\"?\\n    *   This helps you track progress and identify areas for improvement.\\n\\n3.  **Analyze the \"Why\":**\\n    *   Why do you want to increase it? What\\'s your motivation? Understanding this can fuel your efforts.\\n\\n4.  **Break It Down:**\\n    *   Once you have a clear goal, break it into smaller, manageable steps.\\n    *   What are the individual actions you need to take consistently?\\n\\n5.  **Identify Levers and Obstacles:**\\n    *   **Levers:** What are the things you can do or change that will directly impact \"it\"? (e.g., for income: new skills, networking, negotiation, side hustle).\\n    *   **Obstacles:** What\\'s currently preventing \"it\" from increasing? What are the challenges you need to overcome?\\n\\n6.  **Learn and Acquire Knowledge:**\\n    *   Read books, take courses, watch tutorials, find mentors, talk to experts who have successfully increased \"it.\"\\n\\n7.  **Formulate a Plan/Strategy:**\\n    *   Based on your research and analysis, create an action plan. What will you do, when will you do it, and how often?\\n\\n8.  **Take Consistent Action:**\\n    *   Execution is key. A plan without action is just a wish. Consistency over time yields results.\\n\\n9.  **Track Progress and Adjust:**\\n    *   Regularly monitor your progress against your baseline and goals.\\n    *   If something isn\\'t working, don\\'t be afraid to adjust your strategy. Learn from what\\'s not working.\\n\\n10. **Stay Persistent and Patient:**\\n    *   Most significant increases take time and effort. There will be setbacks. Resilience is crucial.\\n\\n---\\n\\n**To give you a better idea, here are some common things people want to increase, and general approaches for each:**\\n\\n*   **Income/Money:**\\n    *   **Earn More:** Ask for a raise, get a promotion, find a higher-paying job, start a side hustle, invest in new skills, start a business.\\n    *   **Save More:** Create a budget, cut unnecessary expenses, automate savings.\\n    *   **Invest More:** Learn about investing, diversify your portfolio.\\n*   **Knowledge/Skills:**\\n    *   Read books, take online courses, practice regularly, seek mentorship, attend workshops.\\n*   **Physical Fitness/Muscle Mass:**\\n    *   Follow a structured workout plan (strength training, cardio), optimize nutrition (protein intake), ensure adequate sleep, stay hydrated.\\n*   **Happiness/Well-being:**\\n    *   Practice gratitude, mindfulness/meditation, spend time in nature, nurture relationships, pursue hobbies, help others, limit negative influences.\\n*   **Productivity:**\\n    *   Time management techniques (Pomodoro, time blocking), prioritize tasks (Eisenhower Matrix), minimize distractions, take regular breaks, optimize your workspace.\\n*   **Sales/Business Revenue:**\\n    *   Improve marketing, enhance customer service, optimize pricing, expand product/service offerings, enter new markets, improve sales techniques.\\n\\nPlease tell me what \"it\" is, and I can give you much more tailored and actionable advice!', additional_kwargs={}, response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'model_name': 'gemini-2.5-flash', 'safety_ratings': []}, id='run--54c241c2-dcc1-4a4e-bc8f-95e9a18334ee-0', usage_metadata={'input_tokens': 7, 'output_tokens': 2054, 'total_tokens': 2061, 'input_token_details': {'cache_read': 0}, 'output_token_details': {'reasoning': 1140}}), HumanMessage(content='What is my name?', additional_kwargs={}, response_metadata={}), AIMessage(content=\"As an AI, I don't know your name or any personal information about you. I don't have memory of past conversations or access to your identity.\\n\\nYou would need to tell me your name for me to know it!\", additional_kwargs={}, response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'model_name': 'gemini-2.5-flash', 'safety_ratings': []}, id='run--18795d81-3635-4ad8-adcc-6e733c7e6775-0', usage_metadata={'input_tokens': 928, 'output_tokens': 88, 'total_tokens': 1016, 'input_token_details': {'cache_read': 0}, 'output_token_details': {'reasoning': 40}})]),\n",
       " 'third_chat': InMemoryChatMessageHistory(messages=[HumanMessage(content='What is my name', additional_kwargs={}, response_metadata={}), AIMessage(content=\"As an AI, I don't know your name. I don't have access to personal information about you unless you choose to tell me.\", additional_kwargs={}, response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'model_name': 'gemini-2.5-flash', 'safety_ratings': []}, id='run--562f7b03-b057-48ef-a87b-db6338635ffc-0', usage_metadata={'input_tokens': 20, 'output_tokens': 262, 'total_tokens': 282, 'input_token_details': {'cache_read': 0}, 'output_token_details': {'reasoning': 232}})])}"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "fc3e16b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import trim_messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "4e4484ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "trimmer=trim_messages(max_tokens=50,\n",
    "                      strategy='last',\n",
    "                      token_counter=llm,\n",
    "                      include_system=True,\n",
    "                      allow_partial=False,\n",
    "                      start_on=\"human\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "95e4d014",
   "metadata": {},
   "outputs": [],
   "source": [
    "messages=[\n",
    "    HumanMessage(content=\"Hello I am Tapas\"),\n",
    "    AIMessage(content=\"Hey Tapas, How can I help you?\"),\n",
    "    HumanMessage(content=\"What is 2+2?\"),\n",
    "    AIMessage(content=\"Hey Tapas, 2+2 is 4\"),\n",
    "    HumanMessage(content=\"How was the movie 3-idiots\"),\n",
    "    AIMessage(content=\"Hey Tapas, 3-idiots is a comedy movie\"),\n",
    "    HumanMessage(content=\"What is 3*3?\"),\n",
    "    AIMessage(content=\"3 multiply by 3 is 9\"),\n",
    "    HumanMessage(content=\"Bye\"),\n",
    "    AIMessage(content=\"Bye ,have a good day\")\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "5301d700",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "98"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm.get_num_tokens_from_messages(messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "8a10b796",
   "metadata": {},
   "outputs": [],
   "source": [
    "trimmed_message=trimmer.invoke(messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "da6592e4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "31"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm.get_num_tokens_from_messages(trimmed_message)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "6dcb2571",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[HumanMessage(content='What is 3*3?', additional_kwargs={}, response_metadata={}),\n",
       " AIMessage(content='3 multiply by 3 is 9', additional_kwargs={}, response_metadata={}),\n",
       " HumanMessage(content='Bye', additional_kwargs={}, response_metadata={}),\n",
       " AIMessage(content='Bye ,have a good day', additional_kwargs={}, response_metadata={})]"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trimmed_message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "6072ae1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from operator import itemgetter\n",
    "from langchain_core.runnables import RunnablePassthrough"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "f8fa11c2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptTemplate(input_variables=['messages'], input_types={'messages': list[typing.Annotated[typing.Union[typing.Annotated[langchain_core.messages.ai.AIMessage, Tag(tag='ai')], typing.Annotated[langchain_core.messages.human.HumanMessage, Tag(tag='human')], typing.Annotated[langchain_core.messages.chat.ChatMessage, Tag(tag='chat')], typing.Annotated[langchain_core.messages.system.SystemMessage, Tag(tag='system')], typing.Annotated[langchain_core.messages.function.FunctionMessage, Tag(tag='function')], typing.Annotated[langchain_core.messages.tool.ToolMessage, Tag(tag='tool')], typing.Annotated[langchain_core.messages.ai.AIMessageChunk, Tag(tag='AIMessageChunk')], typing.Annotated[langchain_core.messages.human.HumanMessageChunk, Tag(tag='HumanMessageChunk')], typing.Annotated[langchain_core.messages.chat.ChatMessageChunk, Tag(tag='ChatMessageChunk')], typing.Annotated[langchain_core.messages.system.SystemMessageChunk, Tag(tag='SystemMessageChunk')], typing.Annotated[langchain_core.messages.function.FunctionMessageChunk, Tag(tag='FunctionMessageChunk')], typing.Annotated[langchain_core.messages.tool.ToolMessageChunk, Tag(tag='ToolMessageChunk')]], FieldInfo(annotation=NoneType, required=True, discriminator=Discriminator(discriminator=<function _get_type at 0x000001CD5FD4E7A0>, custom_error_type=None, custom_error_message=None, custom_error_context=None))]]}, partial_variables={}, messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], input_types={}, partial_variables={}, template='You are a helpful assistant. Answer all questions to the best of your ability'), additional_kwargs={}), MessagesPlaceholder(variable_name='messages')])"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "48d46619",
   "metadata": {},
   "outputs": [],
   "source": [
    "chain=(\n",
    "    RunnablePassthrough.assign(messages=itemgetter(\"messages\") | trimmer)\n",
    "    | prompt\n",
    "    | llm   \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "ef7d7968",
   "metadata": {},
   "outputs": [],
   "source": [
    "response=chain.invoke(\n",
    "    {\n",
    "        \"messages\": messages + [HumanMessage(content=\"What is my last question?\")],\n",
    "        \n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "3442732c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Your last question was \"What is 3*3?\".', additional_kwargs={}, response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'model_name': 'gemini-2.5-flash', 'safety_ratings': []}, id='run--526653ef-8e44-49ea-ba4d-b81b7e5ed974-0', usage_metadata={'input_tokens': 48, 'output_tokens': 170, 'total_tokens': 218, 'input_token_details': {'cache_read': 0}, 'output_token_details': {'reasoning': 158}})"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "923d4fac",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_with_memory=RunnableWithMessageHistory(\n",
    "    chain,\n",
    "    get_session_history,\n",
    "    input_message_key=\"messages\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "8d3bd487",
   "metadata": {},
   "outputs": [],
   "source": [
    "config={\"configurable\":{\"session_id\":\"forth_chat\"}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "11be79f2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[HumanMessage(content='Hello I am Tapas', additional_kwargs={}, response_metadata={}),\n",
       " AIMessage(content='Hey Tapas, How can I help you?', additional_kwargs={}, response_metadata={}),\n",
       " HumanMessage(content='What is 2+2?', additional_kwargs={}, response_metadata={}),\n",
       " AIMessage(content='Hey Tapas, 2+2 is 4', additional_kwargs={}, response_metadata={}),\n",
       " HumanMessage(content='How was the movie 3-idiots', additional_kwargs={}, response_metadata={}),\n",
       " AIMessage(content='Hey Tapas, 3-idiots is a comedy movie', additional_kwargs={}, response_metadata={}),\n",
       " HumanMessage(content='What is 3*3?', additional_kwargs={}, response_metadata={}),\n",
       " AIMessage(content='3 multiply by 3 is 9', additional_kwargs={}, response_metadata={}),\n",
       " HumanMessage(content='Bye', additional_kwargs={}, response_metadata={}),\n",
       " AIMessage(content='Bye ,have a good day', additional_kwargs={}, response_metadata={})]"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2ecc405",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = model_with_memory.invoke(\n",
    "    {\n",
    "        \"messages\": messages + [HumanMessage(content=\"whats my name?\")],\n",
    "        \"language\": \"English\",\n",
    "    },\n",
    "    config=config,\n",
    ")\n",
    "\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68e1a0a8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "genai_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
