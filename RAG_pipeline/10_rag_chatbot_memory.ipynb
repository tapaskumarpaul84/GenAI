{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "396de132",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "all ok\n"
     ]
    }
   ],
   "source": [
    "print(\"all ok\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f4855c61",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "os.environ[\"GOOGLE_API_KEY\"]=os.getenv(\"GOOGLE_API_KEY\")\n",
    "os.environ[\"OPENAI_API_KEY\"]=os.getenv(\"OPENAI_API_KEY\")\n",
    "os.environ[\"LANGSMITH_TRACING\"]=os.getenv(\"LANGSMITH_TRACING\")\n",
    "os.environ[\"LANGSMITH_ENDPOINT\"]=os.getenv(\"LANGSMITH_ENDPOINT\")\n",
    "os.environ[\"LANGSMITH_API_KEY\"]=os.getenv(\"LANGSMITH_API_KEY\")\n",
    "os.environ[\"LANGSMITH_PROJECT\"]=os.getenv(\"LANGSMITH_PROJECT\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0fd33f32",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai.embeddings import OpenAIEmbeddings\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "embeddings=OpenAIEmbeddings()\n",
    "llm = ChatGoogleGenerativeAI(model=\"gemini-2.5-flash\",convert_system_message_to_human=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bfe15ac5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "USER_AGENT environment variable not set, consider setting it to identify your requests.\n"
     ]
    }
   ],
   "source": [
    "import bs4\n",
    "from langchain import hub\n",
    "from langchain.chains import create_retrieval_chain\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_core.prompts import MessagesPlaceholder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5cb23a11",
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = WebBaseLoader(web_paths=(\"https://lilianweng.github.io/posts/2023-06-23-agent/\",),bs_kwargs=dict(parse_only=bs4.SoupStrainer(class_=(\"post-content\", \"post-title\", \"post-header\"))),)\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5ef8444d",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc=loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1c6504cf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4316a05a",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_spliter=RecursiveCharacterTextSplitter(chunk_size=500,chunk_overlap=50)\n",
    "docs=text_spliter.split_documents(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2eb06972",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "134"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "96b981a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}, page_content='LLM Powered Autonomous Agents\\n    \\nDate: June 23, 2023  |  Estimated Reading Time: 31 min  |  Author: Lilian Weng')"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "420b2004",
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_store=Chroma(collection_name=\"testcollection\",embedding_function=embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d216be5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from uuid import uuid4\n",
    "uuids=[str(uuid4()) for _ in range(len(docs))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a097ba6e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['aacacc5e-e438-473f-836d-eaf5a4d07e1c',\n",
       " '489e1582-da56-4045-9720-7e61cc939c9a',\n",
       " 'a939a089-fed0-4215-81bb-a2f781dce584',\n",
       " 'c455b210-aca4-4398-aa92-ae6623d14848',\n",
       " 'e49ae61a-37b2-4b0c-b44a-600a09081baa',\n",
       " 'bd575608-4fe5-4927-96d5-634f112d3661',\n",
       " '59d068e2-3ae9-4577-bfe7-7f7854672394',\n",
       " 'a5250610-2f36-4863-ad32-6cf4771d039b',\n",
       " '733ab894-0652-4a20-a5a1-48b5f67dfc41',\n",
       " '6ccb34c6-d545-4320-b803-625707a058fe',\n",
       " '3614ee9c-898c-4646-837c-9c5941a2601e',\n",
       " '7fa757b0-b435-43f3-b995-592b99333260',\n",
       " 'f0245754-150e-4bc1-84df-4dda256471e0',\n",
       " 'a807cc25-f319-4876-ac00-1bd9ce8f9419',\n",
       " 'ecbe5bfd-65a6-4b23-a2ca-fe6aa57f6a32',\n",
       " '8bdee82a-b28a-49cc-aa63-c031912ac274',\n",
       " '9cfad3e1-ed38-4b5b-b780-2cc6b209ae9a',\n",
       " 'b582c6ee-2418-4b72-bd0d-fe02a974625b',\n",
       " '52a0e0a2-4302-4d8f-a5c6-39affbfb5ea0',\n",
       " '6b5b40c6-9343-4883-b3c5-d3af491ea1a8',\n",
       " 'fc4a5f52-3717-4047-bf1a-564fff7d10eb',\n",
       " '422444cf-aeff-4976-ba2e-c7db7225838c',\n",
       " '82d4508e-2fa7-4078-9942-5664cc87b94e',\n",
       " 'b8f7ade0-b91d-4b33-99f0-8ecd24cf28b4',\n",
       " '11bffe6f-e472-4f28-bd4b-8bb30a68b584',\n",
       " '68eeb12d-cee5-4b0c-a01f-99bf3f690e7d',\n",
       " '655b8b90-7547-4945-b183-df9bcbb35992',\n",
       " '5088cf0c-0399-4997-94fe-2062f7d7b14d',\n",
       " '5a228ba4-e138-41c2-9724-677beeb2bb0e',\n",
       " '614335ca-9bff-4d5a-9f46-621093132fcf',\n",
       " '6fb8a60e-b88b-4d4f-93e5-97dc538c3152',\n",
       " '62c4be07-5001-49e5-9b0c-1c032e215c79',\n",
       " '09a97da9-807b-4b52-95c6-647bbcf814ec',\n",
       " '195bf83e-6a07-4ba1-8e03-1af4655dc59b',\n",
       " '61bd285f-81e2-4668-a193-ce32489eacd3',\n",
       " 'f6f0d419-c206-4295-8954-fdbc4c53bf5b',\n",
       " 'd77eb794-a592-44e8-92af-35ddce69ee44',\n",
       " '36fbd120-6484-4978-89c6-35abe7516afd',\n",
       " '0a10fc85-0cde-4706-b7b3-03974e783245',\n",
       " 'ac2d3f85-3230-4cef-b668-9cc490f94d31',\n",
       " 'd4c15788-c3cb-4adc-a49e-3ae20d99f228',\n",
       " '46c28c08-a288-4783-ac04-1d04649fd916',\n",
       " 'cb21e9a4-fbe9-45de-811d-54cd55ff93a3',\n",
       " 'cbf36bd3-89ee-47f5-b421-eafdfa312fb6',\n",
       " '9d9124c7-2945-465a-87bf-fa4dcd1ddfe1',\n",
       " '15a45179-7efa-4e31-815d-ad66e9bda1d1',\n",
       " 'd8dc8cee-115b-4a20-a963-8aa62c818450',\n",
       " '1b1475c5-8427-4c4f-aa63-dfff955498e9',\n",
       " '9cc640d4-8885-44e4-bc3f-c6cd45883352',\n",
       " '2ce2be39-57e8-460f-8d58-dd4647db880a',\n",
       " '0fc21037-f58a-4dfe-979e-35ca218b7f00',\n",
       " 'fe4cb7ed-ed26-41e9-9377-3ea84b820a78',\n",
       " '1b47a3bb-376c-4486-ad1c-c9eca4c88d93',\n",
       " 'f352f03d-76bf-44bb-b040-d2b91c312e49',\n",
       " 'e20cf5ec-a84a-4402-b070-166e5ea43b7d',\n",
       " '3f5feeea-9d5c-431b-a226-75a05246f689',\n",
       " 'ca9fca8f-ba93-4086-b71d-5b8243583405',\n",
       " '336bdeee-b9fd-4411-b6aa-0b784ea7d8db',\n",
       " '4de6b693-7144-49fb-8284-e9a37f8b2e46',\n",
       " 'c3435b28-c58b-4994-9f20-f84c1ba59c22',\n",
       " 'fa735805-1255-42fd-b3c6-413b0d9d2f78',\n",
       " '85f212e1-6c0e-42f5-a268-0608c9f9dd9b',\n",
       " '3f86b02e-9594-471e-a336-4b5e921100df',\n",
       " 'd0eed2d4-d96f-4155-84e8-6e71d373ac07',\n",
       " '96effc0d-d380-452f-afde-268ffdca41b7',\n",
       " 'd84f1e33-965c-4d80-b97b-efed847e7235',\n",
       " '1c5d283a-3f02-4c4f-b888-ad585e7e3cea',\n",
       " 'ad4e5bc4-a886-4e22-8f9c-f8181695dd7a',\n",
       " '798ba160-ef0f-4b52-a034-784cbf582cb8',\n",
       " '1f4107cc-9f91-4d5d-88cd-6fc332f9aba9',\n",
       " '00990fee-3a82-4d52-b824-67ff48af9329',\n",
       " '46e8a4b6-c629-46f4-9f8e-454ae6164355',\n",
       " '1d5bf4a3-ba10-4e8e-9474-4d930ed00512',\n",
       " '6c75d82f-1ef0-4ff6-9486-542afe64efed',\n",
       " '5f5df541-203b-4c12-885c-e054b08cd376',\n",
       " 'fe96b1b1-a7b5-4605-8634-6e1a3c9028f7',\n",
       " '977a6051-bc7c-43ed-a6f8-6392afc3f0d2',\n",
       " '3127b4b7-3c1d-47cd-b807-e00a8e9dbf0e',\n",
       " '19078363-38c2-4806-ad94-6bb3e704be6c',\n",
       " '945b82b2-7ac0-451a-880f-eba0fc7fd106',\n",
       " '60a1bd5f-022b-4ee9-b8ab-551208aa20a1',\n",
       " '4d84def3-ae30-4e59-a063-e342a1827ccb',\n",
       " '5946545c-7181-4f8e-ba50-bbd1503b5217',\n",
       " 'ea78e58d-c74f-4ff4-884a-cd7aa1df43e8',\n",
       " '106e7af2-8b2b-4f16-b669-b33c2bc7f9c0',\n",
       " '9a49eb81-caaf-48a9-a0da-e8a8ae769a10',\n",
       " '2bcfce56-a9d3-4d51-8883-cb355ae34d95',\n",
       " '0a1e5fce-8c6d-401d-8bb8-6cd6f5be835a',\n",
       " '0f6d4280-3d6b-493c-aaa1-0d33f8df3982',\n",
       " 'c90da0e1-bac7-4f6a-8f0a-4b5a68eaca1d',\n",
       " '62bc8bb3-cc94-469c-b388-6d2c57876296',\n",
       " '916ea59f-842b-44b4-8a84-a41f6af1a3f2',\n",
       " '981d561b-9327-4c16-8e39-1e040b17a2c9',\n",
       " 'ca4f4b1a-2057-42df-8d1d-aa5a7ee027f2',\n",
       " 'c22984e4-3d49-41b7-8c68-b7518af6e4f4',\n",
       " '14296351-82ee-4936-9341-4e9029b359a7',\n",
       " '34d5b56b-c715-4c2b-99f0-28bac5b718c7',\n",
       " 'f6cb91e0-b47b-4e97-b46e-0bdc96195aa1',\n",
       " '76f0f6e9-c8bf-4cca-ad1b-3f4faa1200e0',\n",
       " '2f77b844-b19c-4307-a292-642fd3a59a5a',\n",
       " '2a9162ac-94ea-4e7e-a48e-4884794ad87f',\n",
       " 'e2704ca5-de87-46e2-afcd-d36e8e0bedcd',\n",
       " '0484b86a-a12e-40b9-8bd9-36ca7bff5e06',\n",
       " 'cd9bb853-7697-4346-9733-acb74de6f23c',\n",
       " '70678794-7d47-4730-9f59-611ba2fd96b0',\n",
       " '17107649-21de-4b20-ae14-4edbedff6d87',\n",
       " 'a1f56e9a-1fc1-4453-ae79-49f5b3953256',\n",
       " '1c9bd42a-222e-4794-9834-18568772f9d0',\n",
       " '19df19cf-b56f-43e7-8dec-825599ebb97b',\n",
       " '62590bdd-7c00-449d-a2c2-e92d72ecf2ba',\n",
       " '78a5795f-34b1-4def-b34d-03dca00ff346',\n",
       " 'f0ac02c1-ad5e-4592-b997-b396d1dd45b2',\n",
       " 'b6a8b353-4f3c-4bc1-8098-cd815a80144d',\n",
       " '0405cc87-8110-4c1f-a3f4-4435a2552bc1',\n",
       " '88c2a741-94d2-4ded-b46f-64d3e5f0a960',\n",
       " '22b756f9-39ac-4016-8d8f-9ace996ca472',\n",
       " '1298b8bf-4ec9-4ec2-a8fb-c47bec251039',\n",
       " '1bed4f73-6fe2-4e2c-9e7d-75da9dee7b2a',\n",
       " 'ded91a9c-d233-422b-80fb-16ab15651101',\n",
       " 'b087967a-a43e-43a2-b5d7-4d3123bde02b',\n",
       " '68d78ce6-77a9-4b91-a0d1-be487c6eacda',\n",
       " '7b919796-a7bc-4673-ab7e-38df721a3aba',\n",
       " 'f448bd11-ef2f-4ee6-80ff-835fe84257b6',\n",
       " 'd7b5e176-282c-4151-8032-5ed696a1ce69',\n",
       " 'f2343d6f-f2ac-43b9-bc20-9ce6e1d13eb8',\n",
       " '268429a1-858c-49d5-88c2-2835b09a0166',\n",
       " '80627795-e320-4626-b16e-453f3ed3f661',\n",
       " 'ddd88e2b-bfcd-41a7-ac15-d0000d8f17a3',\n",
       " '3562a98a-454c-4744-84b3-07b28c1cb9ee',\n",
       " '1290a23c-6501-4ead-b673-5d084ef00113',\n",
       " 'abee849a-9802-40cb-b335-a5b525dbdec7',\n",
       " '4f9e4c3a-216c-4851-9e0a-bcb1ce183107',\n",
       " '8cad9b07-95bd-43d3-b9e6-643a4cdaaef0',\n",
       " '905f0bdc-f916-449b-b243-8317ade06151']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vector_store.add_documents(documents=docs,ids=uuids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "24175aef",
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever=vector_store.as_retriever(search_kwargs={\"k\":5})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ba9d4105",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt=\"\"\"\n",
    "You are an helpful assistant for question-answering task.\n",
    "Use the following retrieve context to answer the user question.\n",
    "Use maximum 3-4 sentences for your response and response should be consise and meaningful.\n",
    "If you do not find the response then simply tell that you don't know.\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5f111c69",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt=ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\",system_prompt),\n",
    "        (\"human\",\"{input}\")\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "510259ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "question_answering_chain=create_stuff_documents_chain(llm,prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "730ac735",
   "metadata": {},
   "outputs": [],
   "source": [
    "rag_chain=create_retrieval_chain(retriever,question_answering_chain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ce1765ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Hp\\miniconda3\\envs\\genai_venv\\Lib\\site-packages\\langchain_google_genai\\chat_models.py:499: UserWarning: Convert_system_message_to_human will be deprecated!\n",
      "  warnings.warn(\"Convert_system_message_to_human will be deprecated!\")\n"
     ]
    }
   ],
   "source": [
    "response=rag_chain.invoke({\"input\":\"what is MRKL\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "fe6f98ed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'MRKL, which stands for “Modular Reasoning, Knowledge and Language,” is a neuro-symbolic architecture designed for autonomous agents. It comprises a collection of \"expert\" modules, where a general-purpose Large Language Model (LLM) functions as a router to direct inquiries to the most suitable expert. These modules can be either neural, like deep learning models, or symbolic, such as math calculators, currency converters, or weather APIs.'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response['answer']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "4d7ccaa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_core.output_parsers import StrOutputParser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a7e1c2e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "lcel_chain=(\n",
    "    {\"context\":retriever,\"input\":RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "674bf194",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Hp\\miniconda3\\envs\\genai_venv\\Lib\\site-packages\\langchain_google_genai\\chat_models.py:499: UserWarning: Convert_system_message_to_human will be deprecated!\n",
      "  warnings.warn(\"Convert_system_message_to_human will be deprecated!\")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'MRKL, short for “Modular Reasoning, Knowledge and Language,” is a neuro-symbolic architecture designed for autonomous agents. It combines a general-purpose Large Language Model (LLM) acting as a router with a collection of \"expert\" modules. These modules can be either neural, like deep learning models, or symbolic, such as math calculators or weather APIs, allowing the system to route inquiries to the most suitable expert.'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res=lcel_chain.invoke(\"what is MRKL\")\n",
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "9d1f31bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import create_history_aware_retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "1d483fe0",
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever_prompt=(\n",
    "    \"\"\"\n",
    "Given a chat history and the latest user question which might reference context in the chat\n",
    "history.\n",
    "formulate a standalone question which can be understood without the chat history.\n",
    "Do not answer the question, just formulate it if needed and otherwise return it as is\n",
    "\n",
    "\"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "a171caae",
   "metadata": {},
   "outputs": [],
   "source": [
    "contextualized_prompt=ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\",retriever_prompt),\n",
    "        MessagesPlaceholder(variable_name=\"chat_history\"),\n",
    "        (\"human\",\"{input}\")\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "d29a8505",
   "metadata": {},
   "outputs": [],
   "source": [
    "history_aware_retriever=create_history_aware_retriever(llm,retriever,contextualized_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "bae0e8fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "qa_prompt=ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\",system_prompt),\n",
    "        MessagesPlaceholder(\"chat_history\"),\n",
    "        (\"human\",\"{input}\")\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "d3b2ec17",
   "metadata": {},
   "outputs": [],
   "source": [
    "question_answering_chain=create_stuff_documents_chain(llm,qa_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "bd680b83",
   "metadata": {},
   "outputs": [],
   "source": [
    "rag_chain=create_retrieval_chain(history_aware_retriever,question_answering_chain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "6ad604c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import HumanMessage,AIMessage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "f6821c43",
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_history=[]\n",
    "question_1=\"What is task decomposition?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "81d2896c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Hp\\miniconda3\\envs\\genai_venv\\Lib\\site-packages\\langchain_google_genai\\chat_models.py:499: UserWarning: Convert_system_message_to_human will be deprecated!\n",
      "  warnings.warn(\"Convert_system_message_to_human will be deprecated!\")\n"
     ]
    }
   ],
   "source": [
    "message1=rag_chain.invoke({\"input\": question_1,\"chat_history\":chat_history})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "7a00ccf9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Task decomposition is a standard technique used to break down complex tasks into smaller, simpler, and more manageable steps. This process enhances model performance by allowing the model to \"think step by step\" and efficiently handle intricate problems. It can be achieved through simple LLM prompting, task-specific instructions, or human input.'"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "message1[\"answer\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "a291d0ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_history.extend(\n",
    "    [\n",
    "        HumanMessage(content=question_1),\n",
    "        AIMessage(content=message1['answer'])\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "d96582a0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[HumanMessage(content='What is task decomposition?', additional_kwargs={}, response_metadata={}),\n",
       " AIMessage(content='Task decomposition is a standard technique used to break down complex tasks into smaller, simpler, and more manageable steps. This process enhances model performance by allowing the model to \"think step by step\" and efficiently handle intricate problems. It can be achieved through simple LLM prompting, task-specific instructions, or human input.', additional_kwargs={}, response_metadata={})]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "13349de1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Hp\\miniconda3\\envs\\genai_venv\\Lib\\site-packages\\langchain_google_genai\\chat_models.py:499: UserWarning: Convert_system_message_to_human will be deprecated!\n",
      "  warnings.warn(\"Convert_system_message_to_human will be deprecated!\")\n",
      "c:\\Users\\Hp\\miniconda3\\envs\\genai_venv\\Lib\\site-packages\\langchain_google_genai\\chat_models.py:499: UserWarning: Convert_system_message_to_human will be deprecated!\n",
      "  warnings.warn(\"Convert_system_message_to_human will be deprecated!\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Task decomposition can be commonly done in three ways. First, by using Large Language Models (LLMs) with simple prompts such as \"Steps for XYZ\" or \"What are the subgoals for achieving XYZ?\". Second, through task-specific instructions, like \"Write a story outline\" for a novel. Lastly, it can also be achieved with human inputs.\n"
     ]
    }
   ],
   "source": [
    "second_question=\"what are the common ways of doing it?\"\n",
    "message2=rag_chain.invoke({\"input\":second_question,\"chat_history\":chat_history})\n",
    "print(message2[\"answer\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "317542f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.chat_message_histories import ChatMessageHistory\n",
    "from langchain_core.chat_history import BaseChatMessageHistory\n",
    "from langchain_core.runnables.history import RunnableWithMessageHistory\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "50d709eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "store={}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "49b5bdac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_session_history(session_id:str)-> BaseChatMessageHistory:\n",
    "    if session_id not in store:\n",
    "        store[session_id]=ChatMessageHistory()\n",
    "    return store[session_id]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "e1ae6045",
   "metadata": {},
   "outputs": [],
   "source": [
    "conversational_rag_chain=RunnableWithMessageHistory(\n",
    "    rag_chain,\n",
    "    get_session_history,\n",
    "    input_messages_key=\"input\",\n",
    "    history_messages_key=\"chat_history\",\n",
    "    output_messages_key=\"answer\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "cdc35eff",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Hp\\miniconda3\\envs\\genai_venv\\Lib\\site-packages\\langchain_google_genai\\chat_models.py:499: UserWarning: Convert_system_message_to_human will be deprecated!\n",
      "  warnings.warn(\"Convert_system_message_to_human will be deprecated!\")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Task decomposition is a technique where large, complex tasks are broken down into smaller, more manageable subgoals or steps. This process enhances model performance on difficult tasks by allowing the model to \"think step by step.\" It can be achieved through simple LLM prompting, task-specific instructions, or human input, making complex tasks more efficient to handle.'"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversational_rag_chain.invoke(\n",
    "    {\"input\": \"What is task decomposition?\"},\n",
    "    config={\n",
    "        \"configurable\":{\"session_id\":\"abc123\"}\n",
    "    }\n",
    ")[\"answer\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "f4b1ef0a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'abc123': InMemoryChatMessageHistory(messages=[HumanMessage(content='What is task decomposition?', additional_kwargs={}, response_metadata={}), AIMessage(content='Task decomposition is a technique where large, complex tasks are broken down into smaller, more manageable subgoals or steps. This process enhances model performance on difficult tasks by allowing the model to \"think step by step.\" It can be achieved through simple LLM prompting, task-specific instructions, or human input, making complex tasks more efficient to handle.', additional_kwargs={}, response_metadata={})])}"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "6a937197",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Hp\\miniconda3\\envs\\genai_venv\\Lib\\site-packages\\langchain_google_genai\\chat_models.py:499: UserWarning: Convert_system_message_to_human will be deprecated!\n",
      "  warnings.warn(\"Convert_system_message_to_human will be deprecated!\")\n",
      "c:\\Users\\Hp\\miniconda3\\envs\\genai_venv\\Lib\\site-packages\\langchain_google_genai\\chat_models.py:499: UserWarning: Convert_system_message_to_human will be deprecated!\n",
      "  warnings.warn(\"Convert_system_message_to_human will be deprecated!\")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Task decomposition can be done in a few common ways. One method involves using Large Language Models (LLMs) with simple prompting, such as asking for \"Steps for XYZ\" or \"subgoals.\" Another approach uses task-specific instructions, like \"Write a story outline\" for a novel. Additionally, human input can also be utilized to break down complex tasks into smaller steps.'"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversational_rag_chain.invoke(\n",
    "    {\"input\": \"What are the common ways of doing it?\"},\n",
    "    config={\n",
    "        \"configurable\":{\"session_id\":\"abc123\"}\n",
    "    }\n",
    ")[\"answer\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "e81b75d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User: What is task decomposition?\n",
      "AI: Task decomposition is a technique where large, complex tasks are broken down into smaller, more manageable subgoals or steps. This process enhances model performance on difficult tasks by allowing the model to \"think step by step.\" It can be achieved through simple LLM prompting, task-specific instructions, or human input, making complex tasks more efficient to handle.\n",
      "User: What are the common ways of doing it?\n",
      "AI: Task decomposition can be done in a few common ways. One method involves using Large Language Models (LLMs) with simple prompting, such as asking for \"Steps for XYZ\" or \"subgoals.\" Another approach uses task-specific instructions, like \"Write a story outline\" for a novel. Additionally, human input can also be utilized to break down complex tasks into smaller steps.\n"
     ]
    }
   ],
   "source": [
    "for message in store[\"abc123\"].messages:\n",
    "    if isinstance(message,AIMessage):\n",
    "        prefix=\"AI\"\n",
    "    else:\n",
    "        prefix=\"User\"\n",
    "    print(f\"{prefix}: {message.content}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "8648ced4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Hp\\miniconda3\\envs\\genai_venv\\Lib\\site-packages\\langchain_google_genai\\chat_models.py:499: UserWarning: Convert_system_message_to_human will be deprecated!\n",
      "  warnings.warn(\"Convert_system_message_to_human will be deprecated!\")\n",
      "c:\\Users\\Hp\\miniconda3\\envs\\genai_venv\\Lib\\site-packages\\langchain_google_genai\\chat_models.py:499: UserWarning: Convert_system_message_to_human will be deprecated!\n",
      "  warnings.warn(\"Convert_system_message_to_human will be deprecated!\")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\"Steps for XYZ\" is an example of a simple prompting technique used with Large Language Models (LLMs) for task decomposition. By providing such a prompt, the LLM is instructed to break down a complex task (represented by \"XYZ\") into a series of smaller, sequential steps, making the overall task more manageable.'"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversational_rag_chain.invoke(\n",
    "    {\"input\": \"What is a prompt technique like step xyz?\"},\n",
    "    config={\n",
    "        \"configurable\":{\"session_id\":\"abc123\"}\n",
    "    }\n",
    ")[\"answer\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "cbc9660a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'abc123': InMemoryChatMessageHistory(messages=[HumanMessage(content='What is task decomposition?', additional_kwargs={}, response_metadata={}), AIMessage(content='Task decomposition is a technique where large, complex tasks are broken down into smaller, more manageable subgoals or steps. This process enhances model performance on difficult tasks by allowing the model to \"think step by step.\" It can be achieved through simple LLM prompting, task-specific instructions, or human input, making complex tasks more efficient to handle.', additional_kwargs={}, response_metadata={}), HumanMessage(content='What are the common ways of doing it?', additional_kwargs={}, response_metadata={}), AIMessage(content='Task decomposition can be done in a few common ways. One method involves using Large Language Models (LLMs) with simple prompting, such as asking for \"Steps for XYZ\" or \"subgoals.\" Another approach uses task-specific instructions, like \"Write a story outline\" for a novel. Additionally, human input can also be utilized to break down complex tasks into smaller steps.', additional_kwargs={}, response_metadata={}), HumanMessage(content='What is a prompt technique like step xyz?', additional_kwargs={}, response_metadata={}), AIMessage(content='\"Steps for XYZ\" is an example of a simple prompting technique used with Large Language Models (LLMs) for task decomposition. By providing such a prompt, the LLM is instructed to break down a complex task (represented by \"XYZ\") into a series of smaller, sequential steps, making the overall task more manageable.', additional_kwargs={}, response_metadata={})])}"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "529460c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User: What is task decomposition?\n",
      "AI: Task decomposition is a technique where large, complex tasks are broken down into smaller, more manageable subgoals or steps. This process enhances model performance on difficult tasks by allowing the model to \"think step by step.\" It can be achieved through simple LLM prompting, task-specific instructions, or human input, making complex tasks more efficient to handle.\n",
      "User: What are the common ways of doing it?\n",
      "AI: Task decomposition can be done in a few common ways. One method involves using Large Language Models (LLMs) with simple prompting, such as asking for \"Steps for XYZ\" or \"subgoals.\" Another approach uses task-specific instructions, like \"Write a story outline\" for a novel. Additionally, human input can also be utilized to break down complex tasks into smaller steps.\n",
      "User: What is a prompt technique like step xyz?\n",
      "AI: \"Steps for XYZ\" is an example of a simple prompting technique used with Large Language Models (LLMs) for task decomposition. By providing such a prompt, the LLM is instructed to break down a complex task (represented by \"XYZ\") into a series of smaller, sequential steps, making the overall task more manageable.\n"
     ]
    }
   ],
   "source": [
    "for message in store[\"abc123\"].messages:\n",
    "    if isinstance(message,AIMessage):\n",
    "        prefix=\"AI\"\n",
    "    else:\n",
    "        prefix=\"User\"\n",
    "    print(f\"{prefix}: {message.content}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "25990195",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Hp\\miniconda3\\envs\\genai_venv\\Lib\\site-packages\\langchain_google_genai\\chat_models.py:499: UserWarning: Convert_system_message_to_human will be deprecated!\n",
      "  warnings.warn(\"Convert_system_message_to_human will be deprecated!\")\n",
      "c:\\Users\\Hp\\miniconda3\\envs\\genai_venv\\Lib\\site-packages\\langchain_google_genai\\chat_models.py:499: UserWarning: Convert_system_message_to_human will be deprecated!\n",
      "  warnings.warn(\"Convert_system_message_to_human will be deprecated!\")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'I apologize, but the provided context does not contain information about what a RAG (Retrieval Augmented Generation) system is. Therefore, I cannot answer your question based on the given text.'"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversational_rag_chain.invoke(\n",
    "    {\"input\": \"What is RAG system?\"},\n",
    "    config={\n",
    "        \"configurable\":{\"session_id\":\"abc123\"}\n",
    "    }\n",
    ")[\"answer\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c909eef",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "genai_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
